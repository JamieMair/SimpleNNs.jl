<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>API · SimpleNNs.jl</title><script data-outdated-warner src="../assets/warner.js"></script><link rel="canonical" href="https://JamieMair.github.io/SimpleNNs.jl/api/"/><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.045/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.24/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../">SimpleNNs.jl</a></span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li><a class="tocitem" href="../getting_started/">Getting Started</a></li><li><span class="tocitem">Examples</span><ul><li><a class="tocitem" href="../mnist/">MNIST</a></li></ul></li><li class="is-active"><a class="tocitem" href>API</a></li><li><a class="tocitem" href="../function_index/">Index</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>API</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>API</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/JamieMair/SimpleNNs.jl/blob/main/docs/src/api.md#" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="API"><a class="docs-heading-anchor" href="#API">API</a><a id="API-1"></a><a class="docs-heading-anchor-permalink" href="#API" title="Permalink"></a></h1><article class="docstring"><header><a class="docstring-binding" id="SimpleNNs.Conv-Union{Tuple{N}, Tuple{Tuple{Vararg{Int64, N}}, Int64}} where N" href="#SimpleNNs.Conv-Union{Tuple{N}, Tuple{Tuple{Vararg{Int64, N}}, Int64}} where N"><code>SimpleNNs.Conv</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">Conv(kernel_size::NTuple{N, Int}, out_channels::Int; kwargs...)</code></pre><p>A convolutional layer with a given kernel size and specified number of output channels.</p><p>This can automatically infer the number of input channels based on the preceeding layers.</p><p><strong>Keyword Arguments</strong></p><ul><li><code>use_bias</code> (default: <code>Val(true)</code>) - Whether or not to add a bias vector to the output. Wrapped in a <code>Val</code> for optimisation.</li><li><code>activation_fn</code> (default: <code>identity</code>) - A custom activation function. Note that not all functions are supported by backpropagation.</li><li><code>parameter_type</code> (default: <code>Val(Float32)</code>) - The datatype to use for the parameters, wrapped in a <code>Val</code> type.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JamieMair/SimpleNNs.jl/blob/062201df3f2447a4338a6db1c9a6c32ab688d9f9/src/layers/conv.jl#L11-L23">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="SimpleNNs.Dense-Tuple{Integer}" href="#SimpleNNs.Dense-Tuple{Integer}"><code>SimpleNNs.Dense</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">Dense(outputs::Integer; kwargs...)</code></pre><p>A representation of a dense layer. By default this can be constructed by  specifying the desired number of outputs. The input size can be inferred from the rest of the chain when constructing a model.</p><p><strong>Keyword Arguments</strong></p><ul><li><code>use_bias</code> (default: <code>Val(true)</code>) - Whether or not to add a bias vector to the output. Wrapped in a <code>Val</code> for optimisation.</li><li><code>activation_fn</code> (default: <code>identity</code>) - A custom activation function. Note that not all functions are supported by backpropagation.</li><li><code>parameter_type</code> (default: <code>Val(Float32)</code>) - The datatype to use for the parameters, wrapped in a <code>Val</code> type.</li><li><code>inputs</code> (default: <code>Infer()</code>) - Specify the number of inputs, or infer them from the rest of the model.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JamieMair/SimpleNNs.jl/blob/062201df3f2447a4338a6db1c9a6c32ab688d9f9/src/layers/dense.jl#L9-L22">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="SimpleNNs.Flatten" href="#SimpleNNs.Flatten"><code>SimpleNNs.Flatten</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Flatten()</code></pre><p>Flatten the dimensions of the preceeding layer, leaving the batch dimension unaffected. The output should be (<code>k</code> x <code>n</code>) where <code>k</code> is the product of the non-batch dimensions of the previous layer.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JamieMair/SimpleNNs.jl/blob/062201df3f2447a4338a6db1c9a6c32ab688d9f9/src/layers/flatten.jl#L2-L8">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="SimpleNNs.LogitCrossEntropyLoss" href="#SimpleNNs.LogitCrossEntropyLoss"><code>SimpleNNs.LogitCrossEntropyLoss</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">LogitCrossEntropyLoss(targets, num_classes::Int)</code></pre><p>Expects the targets in a single vector containg class labels, which have to be between <code>1</code> and <code>num_classes</code> inclusive.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JamieMair/SimpleNNs.jl/blob/062201df3f2447a4338a6db1c9a6c32ab688d9f9/src/backprop/losses.jl#L12-L16">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="SimpleNNs.MSELoss" href="#SimpleNNs.MSELoss"><code>SimpleNNs.MSELoss</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">MSELoss(targets)</code></pre><p>Expects the targets in the form (<code>K</code> x <code>N</code>) where <code>K</code> is the output dimension (usually 1) and <code>N</code> is the batch size.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JamieMair/SimpleNNs.jl/blob/062201df3f2447a4338a6db1c9a6c32ab688d9f9/src/backprop/losses.jl#L3-L7">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="SimpleNNs.MaxPool-Union{Tuple{Tuple{Vararg{Int64, N}}}, Tuple{N}} where N" href="#SimpleNNs.MaxPool-Union{Tuple{Tuple{Vararg{Int64, N}}}, Tuple{N}} where N"><code>SimpleNNs.MaxPool</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">MaxPool(pool_size::NTuple{N, Int}; kwargs...)</code></pre><p>A convolutional max-pool layer with a given kernel size.</p><p>This can automatically infer the necessary sizes if specified.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JamieMair/SimpleNNs.jl/blob/062201df3f2447a4338a6db1c9a6c32ab688d9f9/src/layers/maxpool.jl#L8-L14">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="SimpleNNs.Static-Tuple{Union{Int64, Tuple{Vararg{T, N}} where {N, T}}}" href="#SimpleNNs.Static-Tuple{Union{Int64, Tuple{Vararg{T, N}} where {N, T}}}"><code>SimpleNNs.Static</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">Static(inputs::Union{Int, NTuple}; kwargs...)</code></pre><p>Used for specifying the input type to a neural network. <code>inputs</code> should  be a single integer for a dense network, representing the number of  features. For a image network, <code>inputs</code> can be a tuple specifying the size of the images in the form (WIDTH, HEIGHT, CHANNELS).</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JamieMair/SimpleNNs.jl/blob/062201df3f2447a4338a6db1c9a6c32ab688d9f9/src/layers/static.jl#L5-L12">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="SimpleNNs.activation_gradient_fn-Union{Tuple{Dense{DT, K, T}}, Tuple{T}, Tuple{K}, Tuple{DT}} where {DT, K, T}" href="#SimpleNNs.activation_gradient_fn-Union{Tuple{Dense{DT, K, T}}, Tuple{T}, Tuple{K}, Tuple{DT}} where {DT, K, T}"><code>SimpleNNs.activation_gradient_fn</code></a> — <span class="docstring-category">Method</span></header><section><div><p>Dertivatives are used to backpropagate the gradients of the layer outputs back to the activations of that layer. To save space, these are calculated exclusively using the outputs of the layer. Instead of functions written as dy/dx=f(x), we  instead write dy/dx = g(y). This can be done for the 3 major functions.</p><p>Whenever <code>y</code> is used below, assume this is a function of the output, not the input.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JamieMair/SimpleNNs.jl/blob/062201df3f2447a4338a6db1c9a6c32ab688d9f9/src/backprop/gradients.jl#L9-L16">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="SimpleNNs.backprop!-NTuple{5, Any}" href="#SimpleNNs.backprop!-NTuple{5, Any}"><code>SimpleNNs.backprop!</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">backprop!(partials_buffer, gradient_buffer, inputs, outputs, layer)</code></pre><p>Backpropagates the partial gradients of the outputs of the current <code>layer</code> into the parameters of the current layer. <code>partial_buffers</code> is used as a buffer for the gradients of the output of this layer. <code>gradient_buffer</code> should be  filled up with the gradients of the parameters of the current layer, using the chain rule. <code>inputs</code> is the array fed into the layer and <code>outputs</code> is the output of this layer in the forward pass. <code>layer</code> is the struct containing information about the layer.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JamieMair/SimpleNNs.jl/blob/062201df3f2447a4338a6db1c9a6c32ab688d9f9/src/backprop/backprop.jl#L6-L16">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="SimpleNNs.chain-Tuple" href="#SimpleNNs.chain-Tuple"><code>SimpleNNs.chain</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">chain(layers...)</code></pre><p>Combines the given layer definitions into a single model and propagates the layer sizes through the network.</p><p>The first layer must always be a <code>Static</code> layer which specifies the feature size. If this is a simple fully connection network, then the first layer should be <code>Static(nf)</code> where <code>nf</code> is the number of features in your input matrix. Do not specify the batch size in this static input.</p><p>The default datatype for most layers is <code>Float32</code>, but this may be changed. The parameters of the entire model must be of the same datatype. This function will create a flat parameter vector for the model which can be accessed using the <a href="#SimpleNNs.parameters-Tuple{SimpleNNs.Model}"><code>parameters</code></a> function.</p><p><strong>Examples</strong></p><p>A simple dense, fully-connected, neural network which has 3 input features:</p><pre><code class="language-julia hljs">model = chain(
    Static(3),
    Dense(10, activation_fn=tanh),
    Dense(10, activation_fn=sigmoid),
    Dense(1, activation_fn=identity),
);</code></pre><p>An example convolutional neural network:</p><pre><code class="language-julia hljs"># Image size is (WIDTH, HEIGHT, CHANNELS)
img_size = (28, 28, 1)
model = chain(
    Static(img_size),
    Conv((5,5), 16; activation_fn=relu),
    MaxPool((2,2)),
    Conv((3,3), 8; activation_fn=relu),
    MaxPool((4,4)),
    Flatten(),
    Dense(10, activation_fn=identity)
)</code></pre><p>See also <a href="#SimpleNNs.Static-Tuple{Union{Int64, Tuple{Vararg{T, N}} where {N, T}}}"><code>Static</code></a>, <a href="#SimpleNNs.Dense-Tuple{Integer}"><code>Dense</code></a>, <a href="#SimpleNNs.Conv-Union{Tuple{N}, Tuple{Tuple{Vararg{Int64, N}}, Int64}} where N"><code>Conv</code></a>, <a href="#SimpleNNs.MaxPool-Union{Tuple{Tuple{Vararg{Int64, N}}}, Tuple{N}} where N"><code>MaxPool</code></a>, <a href="#SimpleNNs.Flatten"><code>Flatten</code></a> and <a href="#SimpleNNs.preallocate-Tuple{SimpleNNs.Model, Integer}"><code>preallocate</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JamieMair/SimpleNNs.jl/blob/062201df3f2447a4338a6db1c9a6c32ab688d9f9/src/SimpleNNs.jl#L30-L71">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="SimpleNNs.get_outputs-Tuple{SimpleNNs.ForwardPassCache}" href="#SimpleNNs.get_outputs-Tuple{SimpleNNs.ForwardPassCache}"><code>SimpleNNs.get_outputs</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">get_outputs(cache::ForwardPassCache)</code></pre><p>Gets the last output from the forward pass buffer.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JamieMair/SimpleNNs.jl/blob/062201df3f2447a4338a6db1c9a6c32ab688d9f9/src/forward/preallocation.jl#L6-L10">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="SimpleNNs.gradients-Tuple{SimpleNNs.BackpropagationCache}" href="#SimpleNNs.gradients-Tuple{SimpleNNs.BackpropagationCache}"><code>SimpleNNs.gradients</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">gradients(cache::BackpropagationCache)</code></pre><p>Extracts the gradient array from the backwards pass buffer, filled from use of the <a href="#SimpleNNs.backprop!-NTuple{5, Any}"><code>backprop!</code></a> function.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JamieMair/SimpleNNs.jl/blob/062201df3f2447a4338a6db1c9a6c32ab688d9f9/src/backprop/preallocation.jl#L36-L40">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="SimpleNNs.parameters-Tuple{SimpleNNs.Model}" href="#SimpleNNs.parameters-Tuple{SimpleNNs.Model}"><code>SimpleNNs.parameters</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">parameters(model::Model)</code></pre><p>Returns the array used to store the parameters of the model.</p><p>Modifying this array will change the parameters of the model.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JamieMair/SimpleNNs.jl/blob/062201df3f2447a4338a6db1c9a6c32ab688d9f9/src/SimpleNNs.jl#L21-L27">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="SimpleNNs.preallocate-Tuple{SimpleNNs.Model, Integer}" href="#SimpleNNs.preallocate-Tuple{SimpleNNs.Model, Integer}"><code>SimpleNNs.preallocate</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">preallocate(model::Model, batch_size::Integer)</code></pre><p>Creates a buffer to store the intermediate layer outputs of a forward pass, along with the input.</p><p>The inputs can be set using <a href="#SimpleNNs.set_inputs!-Tuple{SimpleNNs.ForwardPassCache, Any}"><code>set_inputs!</code></a> and the outputs can be retrieved using <a href="#SimpleNNs.get_outputs-Tuple{SimpleNNs.ForwardPassCache}"><code>get_outputs</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JamieMair/SimpleNNs.jl/blob/062201df3f2447a4338a6db1c9a6c32ab688d9f9/src/forward/preallocation.jl#L28-L34">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="SimpleNNs.preallocate_grads-Tuple{SimpleNNs.Model, Integer}" href="#SimpleNNs.preallocate_grads-Tuple{SimpleNNs.Model, Integer}"><code>SimpleNNs.preallocate_grads</code></a> — <span class="docstring-category">Method</span></header><section><div><p>preallocate<em>grads(model::Model, batch</em>size::Integer)</p><p>Creates a buffer to store the intermediate arrays needed for backpropagation.</p><p>The gradients can be retrieved from the buffer using <a href="#SimpleNNs.gradients-Tuple{SimpleNNs.BackpropagationCache}"><code>gradients</code></a> on the buffer.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JamieMair/SimpleNNs.jl/blob/062201df3f2447a4338a6db1c9a6c32ab688d9f9/src/backprop/preallocation.jl#L7-L13">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="SimpleNNs.pullback!-Tuple{Any, Any, SimpleNNs.AbstractLayer}" href="#SimpleNNs.pullback!-Tuple{Any, Any, SimpleNNs.AbstractLayer}"><code>SimpleNNs.pullback!</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">pullback!(input_partials, output_partials, layer)</code></pre><p>Here, we complete the backpropagation of the partial gradients to the inputs of the current layer. This should be called after <code>backprop!</code>. This method will fill the <code>input_partials</code> buffer with partial gradients calculated via the chain rule from the gradients of the partials from this layer&#39;s output.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JamieMair/SimpleNNs.jl/blob/062201df3f2447a4338a6db1c9a6c32ab688d9f9/src/backprop/backprop.jl#L20-L27">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="SimpleNNs.set_inputs!-Tuple{SimpleNNs.ForwardPassCache, Any}" href="#SimpleNNs.set_inputs!-Tuple{SimpleNNs.ForwardPassCache, Any}"><code>SimpleNNs.set_inputs!</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">set_inputs!(cache::ForwardPassCache, inputs)</code></pre><p>Sets the input array in the forward pass cache.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JamieMair/SimpleNNs.jl/blob/062201df3f2447a4338a6db1c9a6c32ab688d9f9/src/forward/preallocation.jl#L54-L58">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="SimpleNNs.GPU.gpu-Tuple{Any}" href="#SimpleNNs.GPU.gpu-Tuple{Any}"><code>SimpleNNs.GPU.gpu</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">gpu(x)</code></pre><p>Creates a copy of <code>x</code> on the GPU, using CUDA. Works for models created with <a href="#SimpleNNs.chain-Tuple"><code>chain</code></a> or plain arrays.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JamieMair/SimpleNNs.jl/blob/062201df3f2447a4338a6db1c9a6c32ab688d9f9/src/gpu.jl#L28-L32">source</a></section></article></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../mnist/">« MNIST</a><a class="docs-footer-nextpage" href="../function_index/">Index »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.23 on <span class="colophon-date" title="Wednesday 30 August 2023 08:32">Wednesday 30 August 2023</span>. Using Julia version 1.9.3.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
