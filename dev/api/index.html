<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>API · SimpleNNs.jl</title><meta name="title" content="API · SimpleNNs.jl"/><meta property="og:title" content="API · SimpleNNs.jl"/><meta property="twitter:title" content="API · SimpleNNs.jl"/><meta name="description" content="Documentation for SimpleNNs.jl."/><meta property="og:description" content="Documentation for SimpleNNs.jl."/><meta property="twitter:description" content="Documentation for SimpleNNs.jl."/><meta property="og:url" content="https://JamieMair.github.io/SimpleNNs.jl/api/"/><meta property="twitter:url" content="https://JamieMair.github.io/SimpleNNs.jl/api/"/><link rel="canonical" href="https://JamieMair.github.io/SimpleNNs.jl/api/"/><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../search_index.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../">SimpleNNs.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li><a class="tocitem" href="../getting_started/">Getting Started</a></li><li><span class="tocitem">Examples</span><ul><li><a class="tocitem" href="../mnist/">MNIST</a></li></ul></li><li><span class="tocitem">Advanced Topics</span><ul><li><a class="tocitem" href="../initialisation/">Parameter Initialisation</a></li><li><a class="tocitem" href="../gpu_usage/">GPU Usage</a></li><li><a class="tocitem" href="../advanced_usage/">Advanced Usage</a></li></ul></li><li class="is-active"><a class="tocitem" href>API</a></li><li><a class="tocitem" href="../function_index/">Index</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>API</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>API</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/JamieMair/SimpleNNs.jl/blob/main/docs/src/api.md#" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="API"><a class="docs-heading-anchor" href="#API">API</a><a id="API-1"></a><a class="docs-heading-anchor-permalink" href="#API" title="Permalink"></a></h1><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="SimpleNNs.AdamOptimiser-Union{Tuple{AbstractArray{T}}, Tuple{T}} where T" href="#SimpleNNs.AdamOptimiser-Union{Tuple{AbstractArray{T}}, Tuple{T}} where T"><code>SimpleNNs.AdamOptimiser</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">AdamOptimiser(gradients::AbstractArray{T}; lr = Float32(1e-3), beta_1 = 0.9f0, beta_2 = 0.999f0) where {T}</code></pre><p>Create an Adam optimiser for gradient-based parameter updates.</p><p><strong>Arguments</strong></p><ul><li><code>gradients</code> (AbstractArray{T}): Template array matching the shape of gradients to be optimised</li></ul><p><strong>Keyword Arguments</strong></p><ul><li><code>lr</code> (T): Learning rate (default: 1e-3)</li><li><code>beta_1</code> (T): Exponential decay rate for first moment estimates (default: 0.9)</li><li><code>beta_2</code> (T): Exponential decay rate for second moment estimates (default: 0.999)</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JamieMair/SimpleNNs.jl/blob/88e332cc9f5c7968a2e4023df140fc85272fdbb8/src/optimisers/adam.jl#L10-L22">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="SimpleNNs.Conv-Union{Tuple{N}, Tuple{NTuple{N, Int64}, Int64}} where N" href="#SimpleNNs.Conv-Union{Tuple{N}, Tuple{NTuple{N, Int64}, Int64}} where N"><code>SimpleNNs.Conv</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">Conv(kernel_size::NTuple{N, Int}, out_channels::Int; kwargs...)</code></pre><p>A convolutional layer with a given kernel size and specified number of output channels.</p><p>This can automatically infer the number of input channels based on the preceeding layers.</p><p><strong>Keyword Arguments</strong></p><ul><li><code>use_bias</code> (default: <code>Val(true)</code>) - Whether or not to add a bias vector to the output. Wrapped in a <code>Val</code> for optimisation.</li><li><code>activation_fn</code> (default: <code>identity</code>) - A custom activation function. Note that not all functions are supported by backpropagation.</li><li><code>parameter_type</code> (default: <code>Val(Float32)</code>) - The datatype to use for the parameters, wrapped in a <code>Val</code> type.</li><li><code>init</code> (default: <code>GlorotNormal()</code>) - Weight initialisation scheme. See <a href="#SimpleNNs.Initialiser"><code>Initialiser</code></a> for available options.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JamieMair/SimpleNNs.jl/blob/88e332cc9f5c7968a2e4023df140fc85272fdbb8/src/layers/conv.jl#L12-L25">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="SimpleNNs.Dense-Tuple{Integer}" href="#SimpleNNs.Dense-Tuple{Integer}"><code>SimpleNNs.Dense</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">Dense(outputs::Integer; kwargs...)</code></pre><p>A representation of a dense layer. By default this can be constructed by  specifying the desired number of outputs. The input size can be inferred from the rest of the chain when constructing a model.</p><p><strong>Keyword Arguments</strong></p><ul><li><code>use_bias</code> (default: <code>Val(true)</code>) - Whether or not to add a bias vector to the output. Wrapped in a <code>Val</code> for optimisation.</li><li><code>activation_fn</code> (default: <code>identity</code>) - A custom activation function. Note that not all functions are supported by backpropagation.</li><li><code>parameter_type</code> (default: <code>Val(Float32)</code>) - The datatype to use for the parameters, wrapped in a <code>Val</code> type.</li><li><code>inputs</code> (default: <code>Infer()</code>) - Specify the number of inputs, or infer them from the rest of the model.</li><li><code>init</code> (default: <code>GlorotNormal()</code>) - Weight initialisation scheme. See <a href="#SimpleNNs.Initialiser"><code>Initialiser</code></a> for available options.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JamieMair/SimpleNNs.jl/blob/88e332cc9f5c7968a2e4023df140fc85272fdbb8/src/layers/dense.jl#L10-L24">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="SimpleNNs.Flatten" href="#SimpleNNs.Flatten"><code>SimpleNNs.Flatten</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">Flatten()</code></pre><p>Flatten the dimensions of the preceeding layer, leaving the batch dimension unaffected. The output should be (<code>k</code> x <code>n</code>) where <code>k</code> is the product of the non-batch dimensions of the previous layer.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JamieMair/SimpleNNs.jl/blob/88e332cc9f5c7968a2e4023df140fc85272fdbb8/src/layers/flatten.jl#L2-L8">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="SimpleNNs.GlorotNormal" href="#SimpleNNs.GlorotNormal"><code>SimpleNNs.GlorotNormal</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">GlorotNormal()</code></pre><p>Glorot normal initialisation (also called Xavier normal). Samples weights from a normal distribution with mean 0 and  standard deviation √(2 / (fan<em>in + fan</em>out)).</p><p>Best suited for layers with sigmoid or tanh activations.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JamieMair/SimpleNNs.jl/blob/88e332cc9f5c7968a2e4023df140fc85272fdbb8/src/initialisers.jl#L20-L28">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="SimpleNNs.GlorotUniform" href="#SimpleNNs.GlorotUniform"><code>SimpleNNs.GlorotUniform</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">GlorotUniform()</code></pre><p>Glorot uniform initialisation (also called Xavier uniform). Samples weights from a uniform distribution in the range [-limit, limit] where limit = √(6 / (fan<em>in + fan</em>out)).</p><p>Best suited for layers with sigmoid or tanh activations.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JamieMair/SimpleNNs.jl/blob/88e332cc9f5c7968a2e4023df140fc85272fdbb8/src/initialisers.jl#L9-L17">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="SimpleNNs.HeNormal" href="#SimpleNNs.HeNormal"><code>SimpleNNs.HeNormal</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">HeNormal()</code></pre><p>He normal initialisation (also called Kaiming normal). Samples weights from a normal distribution with mean 0 and standard deviation √(2 / fan_in).</p><p>Best suited for layers with ReLU activations.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JamieMair/SimpleNNs.jl/blob/88e332cc9f5c7968a2e4023df140fc85272fdbb8/src/initialisers.jl#L42-L50">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="SimpleNNs.HeUniform" href="#SimpleNNs.HeUniform"><code>SimpleNNs.HeUniform</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">HeUniform()</code></pre><p>He uniform initialisation (also called Kaiming uniform). Samples weights from a uniform distribution in the range [-limit, limit] where limit = √(6 / fan_in).</p><p>Best suited for layers with ReLU activations.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JamieMair/SimpleNNs.jl/blob/88e332cc9f5c7968a2e4023df140fc85272fdbb8/src/initialisers.jl#L31-L39">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="SimpleNNs.Initialiser" href="#SimpleNNs.Initialiser"><code>SimpleNNs.Initialiser</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><p>Abstract type for weight initialisation strategies.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JamieMair/SimpleNNs.jl/blob/88e332cc9f5c7968a2e4023df140fc85272fdbb8/src/initialisers.jl#L4-L6">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="SimpleNNs.LeCunNormal" href="#SimpleNNs.LeCunNormal"><code>SimpleNNs.LeCunNormal</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">LeCunNormal()</code></pre><p>LeCun normal initialisation. Samples weights from a normal distribution with mean 0 and standard deviation √(1 / fan_in).</p><p>Best suited for layers with SELU activations.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JamieMair/SimpleNNs.jl/blob/88e332cc9f5c7968a2e4023df140fc85272fdbb8/src/initialisers.jl#L53-L61">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="SimpleNNs.LogitCrossEntropyLoss" href="#SimpleNNs.LogitCrossEntropyLoss"><code>SimpleNNs.LogitCrossEntropyLoss</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">LogitCrossEntropyLoss(targets, num_classes::Int)</code></pre><p>Expects the targets in a single vector containg class labels, which have to be between <code>1</code> and <code>num_classes</code> inclusive.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JamieMair/SimpleNNs.jl/blob/88e332cc9f5c7968a2e4023df140fc85272fdbb8/src/backprop/losses.jl#L18-L22">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="SimpleNNs.MSELoss" href="#SimpleNNs.MSELoss"><code>SimpleNNs.MSELoss</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">MSELoss(targets)</code></pre><p>Expects the targets in the form (<code>K</code> x <code>N</code>) where <code>K</code> is the output dimension (usually 1) and <code>N</code> is the batch size.</p><p>For efficiency, this is just ∑ (y-̂y)² and NOT scaled by a half.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JamieMair/SimpleNNs.jl/blob/88e332cc9f5c7968a2e4023df140fc85272fdbb8/src/backprop/losses.jl#L3-L9">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="SimpleNNs.MaxPool-Union{Tuple{NTuple{N, Int64}}, Tuple{N}} where N" href="#SimpleNNs.MaxPool-Union{Tuple{NTuple{N, Int64}}, Tuple{N}} where N"><code>SimpleNNs.MaxPool</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">MaxPool(pool_size::NTuple{N, Int}; kwargs...)</code></pre><p>A convolutional max-pool layer with a given kernel size.</p><p>This can automatically infer the necessary sizes if specified.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JamieMair/SimpleNNs.jl/blob/88e332cc9f5c7968a2e4023df140fc85272fdbb8/src/layers/maxpool.jl#L8-L14">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="SimpleNNs.RMSPropOptimiser" href="#SimpleNNs.RMSPropOptimiser"><code>SimpleNNs.RMSPropOptimiser</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">RMSPropOptimiser{T, X&lt;:AbstractArray{T}} &lt;: AbstractOptimiser</code></pre><p>RMSProp optimiser with exponential moving average of squared gradients.</p><p><strong>Fields</strong></p><ul><li><code>lr::T</code>: Learning rate</li><li><code>rho::T</code>: Exponential decay rate for moving average</li><li><code>eps::T</code>: Small constant for numerical stability</li><li><code>v::X</code>: Exponential moving average of squared gradients</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JamieMair/SimpleNNs.jl/blob/88e332cc9f5c7968a2e4023df140fc85272fdbb8/src/optimisers/rmsprop.jl#L1-L11">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="SimpleNNs.RMSPropOptimiser-Union{Tuple{AbstractArray{T}}, Tuple{T}} where T" href="#SimpleNNs.RMSPropOptimiser-Union{Tuple{AbstractArray{T}}, Tuple{T}} where T"><code>SimpleNNs.RMSPropOptimiser</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">RMSPropOptimiser(gradients::AbstractArray{T}; lr = Float32(1e-3), rho = 0.9f0, eps = Float32(1e-8)) where {T}</code></pre><p>Create an RMSProp optimiser for gradient-based parameter updates.</p><p>RMSProp maintains a moving average of squared gradients to adaptively scale the learning rate.</p><p><strong>Arguments</strong></p><ul><li><code>gradients</code> (AbstractArray{T}): Template array matching the shape of gradients to be optimised</li></ul><p><strong>Keyword Arguments</strong></p><ul><li><code>lr</code> (T): Learning rate (default: 1e-3)</li><li><code>rho</code> (T): Exponential decay rate for moving average of squared gradients (default: 0.9)</li><li><code>eps</code> (T): Small constant added to denominator for numerical stability (default: 1e-8)</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">opt = RMSPropOptimiser(gradients; lr=0.001f0, rho=0.9f0)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JamieMair/SimpleNNs.jl/blob/88e332cc9f5c7968a2e4023df140fc85272fdbb8/src/optimisers/rmsprop.jl#L19-L38">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="SimpleNNs.SGDOptimiser" href="#SimpleNNs.SGDOptimiser"><code>SimpleNNs.SGDOptimiser</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">SGDOptimiser{T} &lt;: AbstractOptimiser</code></pre><p>Stochastic Gradient Descent optimiser with optional momentum.</p><p><strong>Fields</strong></p><ul><li><code>lr::T</code>: Learning rate</li><li><code>momentum::T</code>: Momentum coefficient (0.0 for no momentum)</li><li><code>velocity::AbstractArray{T}</code>: Velocity buffer for momentum (internal state)</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JamieMair/SimpleNNs.jl/blob/88e332cc9f5c7968a2e4023df140fc85272fdbb8/src/optimisers/sgd.jl#L1-L10">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="SimpleNNs.SGDOptimiser-Union{Tuple{AbstractArray{T}}, Tuple{T}} where T" href="#SimpleNNs.SGDOptimiser-Union{Tuple{AbstractArray{T}}, Tuple{T}} where T"><code>SimpleNNs.SGDOptimiser</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">SGDOptimiser(gradients::AbstractArray{T}; lr = Float32(1e-3), momentum = 0.0f0) where {T}</code></pre><p>Create an SGD optimiser for gradient-based parameter updates.</p><p><strong>Arguments</strong></p><ul><li><code>gradients</code> (AbstractArray{T}): Template array matching the shape of gradients to be optimised</li></ul><p><strong>Keyword Arguments</strong></p><ul><li><code>lr</code> (T): Learning rate (default: 1e-3)</li><li><code>momentum</code> (T): Momentum coefficient, 0.0 for standard SGD (default: 0.0)</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs"># Standard SGD
opt = SGDOptimiser(gradients; lr=0.01f0)

# SGD with momentum
opt = SGDOptimiser(gradients; lr=0.01f0, momentum=0.9f0)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JamieMair/SimpleNNs.jl/blob/88e332cc9f5c7968a2e4023df140fc85272fdbb8/src/optimisers/sgd.jl#L17-L37">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="SimpleNNs.Static-Tuple{Union{Int64, NTuple{N, T} where {N, T}}}" href="#SimpleNNs.Static-Tuple{Union{Int64, NTuple{N, T} where {N, T}}}"><code>SimpleNNs.Static</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">Static(inputs::Union{Int, NTuple}; kwargs...)</code></pre><p>Used for specifying the input type to a neural network. <code>inputs</code> should  be a single integer for a dense network, representing the number of  features. For a image network, <code>inputs</code> can be a tuple specifying the size of the images in the form (WIDTH, HEIGHT, CHANNELS).</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JamieMair/SimpleNNs.jl/blob/88e332cc9f5c7968a2e4023df140fc85272fdbb8/src/layers/static.jl#L5-L12">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="SimpleNNs.Zeros" href="#SimpleNNs.Zeros"><code>SimpleNNs.Zeros</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">Zeros()</code></pre><p>Initialise all weights to zero. Note: This is generally not recommended for training as it breaks symmetry.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JamieMair/SimpleNNs.jl/blob/88e332cc9f5c7968a2e4023df140fc85272fdbb8/src/initialisers.jl#L64-L69">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Base.deepcopy-Tuple{SimpleNNs.Model}" href="#Base.deepcopy-Tuple{SimpleNNs.Model}"><code>Base.deepcopy</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">Base.deepcopy(model::Model)</code></pre><p>Create a deep copy of the model with its own independent parameter array.</p><p>This function creates a new model with:</p><ul><li>A new parameter array (using <code>similar</code> and <code>copyto!</code>)</li><li>New parameter views for each layer pointing to the new array</li><li>The same layer structure and configuration</li></ul><p>The copied model is completely independent from the original - modifying parameters in one will not affect the other.</p><p><strong>Arguments</strong></p><ul><li><code>model::Model</code>: The model to copy</li></ul><p><strong>Returns</strong></p><p>A new <code>Model</code> with copied parameters and structure.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">model = chain(Static(10), Dense(5))
model_copy = deepcopy(model)

# Modify copy - original unchanged
parameters(model_copy) .= 0.0</code></pre><p>See also <a href="#SimpleNNs.parameters-Tuple{SimpleNNs.Model}"><code>parameters</code></a>, <a href="#SimpleNNs.chain-Tuple"><code>chain</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JamieMair/SimpleNNs.jl/blob/88e332cc9f5c7968a2e4023df140fc85272fdbb8/src/chain.jl#L12-L41">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="SimpleNNs.activation_gradient_fn-Tuple{Any}" href="#SimpleNNs.activation_gradient_fn-Tuple{Any}"><code>SimpleNNs.activation_gradient_fn</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><p>Dertivatives are used to backpropagate the gradients of the layer outputs back to the activations of that layer. To save space, these are calculated exclusively using the outputs of the layer. Instead of functions written as dy/dx=f(x), we  instead write dy/dx = g(y). This can be done for the 3 major functions.</p><p>Whenever <code>y</code> is used below, assume this is a function of the output, not the input.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JamieMair/SimpleNNs.jl/blob/88e332cc9f5c7968a2e4023df140fc85272fdbb8/src/backprop/gradients.jl#L9-L16">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="SimpleNNs.add_loss-Tuple{SimpleNNs.Model, SimpleNNs.AbstractTargetsLayer}" href="#SimpleNNs.add_loss-Tuple{SimpleNNs.Model, SimpleNNs.AbstractTargetsLayer}"><code>SimpleNNs.add_loss</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">add_loss(model::Model, loss_layer::AbstractTargetsLayer)</code></pre><p>Create a new model with the given loss layer appended to the end of the existing model.</p><p>This function reconstructs the entire model chain with the loss layer added as the final layer. The original model&#39;s parameters are copied to the new model.</p><p><strong>Arguments</strong></p><ul><li><code>model::Model</code>: The existing model to extend</li><li><code>loss_layer::AbstractTargetsLayer</code>: The loss layer to append (e.g., <code>BatchCrossEntropyLoss</code>)</li></ul><p><strong>Returns</strong></p><p>A new <code>Model</code> with the loss layer appended.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">model = chain(
    Static(10),
    Dense(32, activation_fn=relu),
    Dense(5, activation_fn=identity)
)

# Add a loss layer
targets = zeros(Int, batch_size)
loss_layer = BatchCrossEntropyLoss(targets=targets, num_classes=5)
model_with_loss = add_loss(model, loss_layer)</code></pre><p>See also <a href="#SimpleNNs.remove_loss-Tuple{SimpleNNs.Model}"><code>remove_loss</code></a>, <a href="#SimpleNNs.has_loss-Tuple{SimpleNNs.Model}"><code>has_loss</code></a>, <a href="#SimpleNNs.get_predictions-Tuple{SimpleNNs.Model, Any}"><code>get_predictions</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JamieMair/SimpleNNs.jl/blob/88e332cc9f5c7968a2e4023df140fc85272fdbb8/src/chain.jl#L215-L245">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="SimpleNNs.backprop!-NTuple{5, Any}" href="#SimpleNNs.backprop!-NTuple{5, Any}"><code>SimpleNNs.backprop!</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">backprop!(partials_buffer, gradient_buffer, inputs, outputs, layer)</code></pre><p>Backpropagates the partial gradients of the outputs of the current <code>layer</code> into the parameters of the current layer. <code>partial_buffers</code> is used as a buffer for the gradients of the output of this layer. <code>gradient_buffer</code> should be  filled up with the gradients of the parameters of the current layer, using the chain rule. <code>inputs</code> is the array fed into the layer and <code>outputs</code> is the output of this layer in the forward pass. <code>layer</code> is the struct containing information about the layer.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JamieMair/SimpleNNs.jl/blob/88e332cc9f5c7968a2e4023df140fc85272fdbb8/src/backprop/backprop.jl#L6-L16">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="SimpleNNs.chain-Tuple" href="#SimpleNNs.chain-Tuple"><code>SimpleNNs.chain</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">chain(layers...)</code></pre><p>Combines the given layer definitions into a single model and propagates the layer sizes through the network.</p><p>The first layer must always be a <code>Static</code> layer which specifies the feature size. If this is a simple fully connection network, then the first layer should be <code>Static(nf)</code> where <code>nf</code> is the number of features in your input matrix. Do not specify the batch size in this static input.</p><p>The default datatype for most layers is <code>Float32</code>, but this may be changed. The parameters of the entire model must be of the same datatype. This function will create a flat parameter vector for the model which can be accessed using the <a href="#SimpleNNs.parameters-Tuple{SimpleNNs.Model}"><code>parameters</code></a> function.</p><p><strong>Examples</strong></p><p>A simple dense, fully-connected, neural network which has 3 input features:</p><pre><code class="language-julia hljs">model = chain(
    Static(3),
    Dense(10, activation_fn=tanh),
    Dense(10, activation_fn=sigmoid),
    Dense(1, activation_fn=identity),
);</code></pre><p>An example convolutional neural network:</p><pre><code class="language-julia hljs"># Image size is (WIDTH, HEIGHT, CHANNELS)
img_size = (28, 28, 1)
model = chain(
    Static(img_size),
    Conv((5,5), 16; activation_fn=relu),
    MaxPool((2,2)),
    Conv((3,3), 8; activation_fn=relu),
    MaxPool((4,4)),
    Flatten(),
    Dense(10, activation_fn=identity)
)</code></pre><p>See also <a href="#SimpleNNs.Static-Tuple{Union{Int64, NTuple{N, T} where {N, T}}}"><code>Static</code></a>, <a href="#SimpleNNs.Dense-Tuple{Integer}"><code>Dense</code></a>, <a href="#SimpleNNs.Conv-Union{Tuple{N}, Tuple{NTuple{N, Int64}, Int64}} where N"><code>Conv</code></a>, <a href="#SimpleNNs.MaxPool-Union{Tuple{NTuple{N, Int64}}, Tuple{N}} where N"><code>MaxPool</code></a>, <a href="#SimpleNNs.Flatten"><code>Flatten</code></a> and <a href="#SimpleNNs.preallocate-Tuple{SimpleNNs.Model, Integer}"><code>preallocate</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JamieMair/SimpleNNs.jl/blob/88e332cc9f5c7968a2e4023df140fc85272fdbb8/src/chain.jl#L66-L107">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="SimpleNNs.forward!-Tuple{SimpleNNs.ForwardPassCache, SimpleNNs.Model}" href="#SimpleNNs.forward!-Tuple{SimpleNNs.ForwardPassCache, SimpleNNs.Model}"><code>SimpleNNs.forward!</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">forward!(cache::ForwardPassCache, model::Model)</code></pre><p>Execute a forward pass through the neural network model.</p><p>This function computes the forward propagation through all layers of the model, storing intermediate results in the pre-allocated cache. This is a zero-allocation operation when used with properly pre-allocated caches.</p><p><strong>Arguments</strong></p><ul><li><code>cache::ForwardPassCache</code>: Pre-allocated cache containing input data and space for intermediate results</li><li><code>model::Model</code>: The neural network model to evaluate</li></ul><p><strong>Returns</strong></p><ul><li>The cache object (for convenience), with updated intermediate and output values</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs"># Create model and data
model = chain(Static(4), Dense(8, activation_fn=relu), Dense(1))
inputs = randn(Float32, 4, 32)  # 32 samples, 4 features each

# Pre-allocate cache and set inputs
cache = preallocate(model, 32)
set_inputs!(cache, inputs)

# Execute forward pass
forward!(cache, model)

# Get outputs
outputs = get_outputs(cache)</code></pre><p><strong>Notes</strong></p><ul><li>Requires pre-allocated cache from <code>preallocate(model, batch_size)</code></li><li>Input data must be set using <code>set_inputs!(cache, inputs)</code> before calling</li><li>This is a mutating operation that modifies the cache in-place</li><li>Designed for zero allocations when properly used</li><li>Works on both CPU and GPU when model and data are on the same device</li></ul><p>See also: <a href="#SimpleNNs.preallocate-Tuple{SimpleNNs.Model, Integer}"><code>preallocate</code></a>, <a href="#SimpleNNs.set_inputs!-Tuple{SimpleNNs.ForwardPassCache, Any}"><code>set_inputs!</code></a>, <a href="#SimpleNNs.get_outputs-Tuple{SimpleNNs.ForwardPassCache}"><code>get_outputs</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JamieMair/SimpleNNs.jl/blob/88e332cc9f5c7968a2e4023df140fc85272fdbb8/src/forward/forward.jl#L19-L60">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="SimpleNNs.get_loss-Tuple{SimpleNNs.Model}" href="#SimpleNNs.get_loss-Tuple{SimpleNNs.Model}"><code>SimpleNNs.get_loss</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">get_loss(model::Model)</code></pre><p>Returns the loss layer if the model has one, otherwise returns <code>nothing</code>.</p><p>If the model has a loss layer (a layer extending <code>AbstractTargetsLayer</code>) as its final layer, this function returns that layer. Otherwise, it returns <code>nothing</code>.</p><p><strong>Returns</strong></p><ul><li>The loss layer if present</li><li><code>nothing</code> if the model has no loss layer</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">model = chain(Static(10), Dense(5, activation_fn=relu))
get_loss(model)  # nothing

model_with_loss = add_loss(model, BatchCrossEntropyLoss(targets=zeros(Int, 32), num_classes=5))
loss_layer = get_loss(model_with_loss)  # Returns the BatchCrossEntropyLoss layer</code></pre><p>See also <a href="#SimpleNNs.has_loss-Tuple{SimpleNNs.Model}"><code>has_loss</code></a>, <a href="#SimpleNNs.add_loss-Tuple{SimpleNNs.Model, SimpleNNs.AbstractTargetsLayer}"><code>add_loss</code></a>, <a href="#SimpleNNs.remove_loss-Tuple{SimpleNNs.Model}"><code>remove_loss</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JamieMair/SimpleNNs.jl/blob/88e332cc9f5c7968a2e4023df140fc85272fdbb8/src/chain.jl#L185-L207">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="SimpleNNs.get_outputs-Tuple{SimpleNNs.ForwardPassCache}" href="#SimpleNNs.get_outputs-Tuple{SimpleNNs.ForwardPassCache}"><code>SimpleNNs.get_outputs</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">get_outputs(cache::ForwardPassCache)</code></pre><p>Gets the last output from the forward pass buffer.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JamieMair/SimpleNNs.jl/blob/88e332cc9f5c7968a2e4023df140fc85272fdbb8/src/forward/preallocation.jl#L6-L10">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="SimpleNNs.get_predictions-Tuple{SimpleNNs.Model, Any}" href="#SimpleNNs.get_predictions-Tuple{SimpleNNs.Model, Any}"><code>SimpleNNs.get_predictions</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">get_predictions(model::Model, forward_cache)</code></pre><p>Extract predictions from the forward cache based on whether the model has a loss layer.</p><p>If the model does not have a loss layer, returns the final output from the cache. If the model has a loss layer, returns the input to the loss layer (i.e., the output of the second-to-last layer).</p><p><strong>Arguments</strong></p><ul><li><code>model::Model</code>: The model that was used for the forward pass</li><li><code>forward_cache</code>: The forward cache containing layer outputs</li></ul><p><strong>Returns</strong></p><p>An array containing the model&#39;s predictions (before the loss computation if applicable).</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">model = chain(Static(10), Dense(5, activation_fn=identity))
forward_cache = preallocate(model, batch_size)
set_inputs!(forward_cache, inputs)
forward!(forward_cache, model)

predictions = get_predictions(model, forward_cache)  # Returns final layer output

# With loss layer
model_with_loss = add_loss(model, loss_layer)
forward_cache_with_loss = preallocate(model_with_loss, batch_size)
set_inputs!(forward_cache_with_loss, inputs)
forward!(forward_cache_with_loss, model_with_loss)

predictions = get_predictions(model_with_loss, forward_cache_with_loss)  # Returns output before loss</code></pre><p>See also <a href="#SimpleNNs.add_loss-Tuple{SimpleNNs.Model, SimpleNNs.AbstractTargetsLayer}"><code>add_loss</code></a>, <a href="#SimpleNNs.remove_loss-Tuple{SimpleNNs.Model}"><code>remove_loss</code></a>, <a href="#SimpleNNs.has_loss-Tuple{SimpleNNs.Model}"><code>has_loss</code></a>, <a href="#SimpleNNs.get_outputs-Tuple{SimpleNNs.ForwardPassCache}"><code>get_outputs</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JamieMair/SimpleNNs.jl/blob/88e332cc9f5c7968a2e4023df140fc85272fdbb8/src/chain.jl#L329-L363">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="SimpleNNs.gpu-Tuple{Any}" href="#SimpleNNs.gpu-Tuple{Any}"><code>SimpleNNs.gpu</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">gpu(x)</code></pre><p>Move data or models to GPU using CUDA. This function requires CUDA.jl, cuDNN.jl, and NNlib.jl  to be loaded before use.</p><p><strong>Arguments</strong></p><ul><li><code>x</code>: The object to move to GPU. Can be a <code>Model</code>, <code>AbstractArray</code>, or other supported types.</li></ul><p><strong>Returns</strong></p><ul><li>GPU version of the input object</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using CUDA, cuDNN, NNlib
using SimpleNNs

# Move model to GPU
model = chain(Static(10), Dense(5))
gpu_model = gpu(model)

# Move array to GPU  
cpu_array = randn(Float32, 10, 32)
gpu_array = gpu(cpu_array)</code></pre><p><strong>Notes</strong></p><ul><li>Requires NVIDIA GPU with CUDA support</li><li>CUDA.jl, cuDNN.jl, and NNlib.jl must be loaded before calling this function</li><li>For models, creates a new model with parameters on GPU</li><li>For arrays, converts to CuArray</li><li>Returns input unchanged with warning for unsupported types</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JamieMair/SimpleNNs.jl/blob/88e332cc9f5c7968a2e4023df140fc85272fdbb8/src/gpu.jl#L1-L33">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="SimpleNNs.gradients-Tuple{SimpleNNs.BackpropagationCache}" href="#SimpleNNs.gradients-Tuple{SimpleNNs.BackpropagationCache}"><code>SimpleNNs.gradients</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">gradients(cache::BackpropagationCache)</code></pre><p>Extracts the gradient array from the backwards pass buffer, filled from use of the <a href="#SimpleNNs.backprop!-NTuple{5, Any}"><code>backprop!</code></a> function.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JamieMair/SimpleNNs.jl/blob/88e332cc9f5c7968a2e4023df140fc85272fdbb8/src/backprop/preallocation.jl#L43-L47">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="SimpleNNs.has_loss-Tuple{SimpleNNs.Model}" href="#SimpleNNs.has_loss-Tuple{SimpleNNs.Model}"><code>SimpleNNs.has_loss</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">has_loss(model::Model)</code></pre><p>Check whether the model has a loss layer (a layer extending <code>AbstractTargetsLayer</code>) as its final layer.</p><p>Returns <code>true</code> if the last layer is a loss layer, <code>false</code> otherwise.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">model = chain(Static(10), Dense(5, activation_fn=relu))
has_loss(model)  # false

model_with_loss = add_loss(model, BatchCrossEntropyLoss(targets=zeros(Int, 32), num_classes=5))
has_loss(model_with_loss)  # true</code></pre><p>See also <a href="#SimpleNNs.add_loss-Tuple{SimpleNNs.Model, SimpleNNs.AbstractTargetsLayer}"><code>add_loss</code></a>, <a href="#SimpleNNs.remove_loss-Tuple{SimpleNNs.Model}"><code>remove_loss</code></a>, <a href="#SimpleNNs.get_predictions-Tuple{SimpleNNs.Model, Any}"><code>get_predictions</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JamieMair/SimpleNNs.jl/blob/88e332cc9f5c7968a2e4023df140fc85272fdbb8/src/chain.jl#L160-L177">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="SimpleNNs.initialise!-Tuple{SimpleNNs.Model}" href="#SimpleNNs.initialise!-Tuple{SimpleNNs.Model}"><code>SimpleNNs.initialise!</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">initialise!(model::Model)</code></pre><p>Initialise the parameters of a model according to each layer&#39;s initialisation scheme.</p><p>This function walks through all layers in the model and initialises their weights and biases according to the initialisation method specified in each layer&#39;s <code>init</code> field.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">model = chain(
    Static(10),
    Dense(64, activation_fn=relu, init=HeNormal()),
    Dense(10, activation_fn=identity, init=GlorotNormal())
)
initialise!(model)</code></pre><p>See also: <a href="#SimpleNNs.GlorotUniform"><code>GlorotUniform</code></a>, <a href="#SimpleNNs.GlorotNormal"><code>GlorotNormal</code></a>, <a href="#SimpleNNs.HeNormal"><code>HeNormal</code></a>, <a href="#SimpleNNs.HeUniform"><code>HeUniform</code></a>, <a href="#SimpleNNs.LeCunNormal"><code>LeCunNormal</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JamieMair/SimpleNNs.jl/blob/88e332cc9f5c7968a2e4023df140fc85272fdbb8/src/SimpleNNs.jl#L15-L34">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="SimpleNNs.parameters-Tuple{SimpleNNs.Model}" href="#SimpleNNs.parameters-Tuple{SimpleNNs.Model}"><code>SimpleNNs.parameters</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">parameters(model::Model)</code></pre><p>Returns the array used to store the parameters of the model.</p><p>Modifying this array will change the parameters of the model.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JamieMair/SimpleNNs.jl/blob/88e332cc9f5c7968a2e4023df140fc85272fdbb8/src/chain.jl#L3-L9">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="SimpleNNs.preallocate-Tuple{SimpleNNs.Model, Integer}" href="#SimpleNNs.preallocate-Tuple{SimpleNNs.Model, Integer}"><code>SimpleNNs.preallocate</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">preallocate(model::Model, batch_size::Integer)</code></pre><p>Creates a buffer to store the intermediate layer outputs of a forward pass, along with the input.</p><p>The inputs can be set using <a href="#SimpleNNs.set_inputs!-Tuple{SimpleNNs.ForwardPassCache, Any}"><code>set_inputs!</code></a> and the outputs can be retrieved using <a href="#SimpleNNs.get_outputs-Tuple{SimpleNNs.ForwardPassCache}"><code>get_outputs</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JamieMair/SimpleNNs.jl/blob/88e332cc9f5c7968a2e4023df140fc85272fdbb8/src/forward/preallocation.jl#L30-L36">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="SimpleNNs.preallocate_grads-Tuple{SimpleNNs.Model, Integer}" href="#SimpleNNs.preallocate_grads-Tuple{SimpleNNs.Model, Integer}"><code>SimpleNNs.preallocate_grads</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><p>preallocate<em>grads(model::Model, batch</em>size::Integer)</p><p>Creates a buffer to store the intermediate arrays needed for backpropagation.</p><p>The gradients can be retrieved from the buffer using <a href="#SimpleNNs.gradients-Tuple{SimpleNNs.BackpropagationCache}"><code>gradients</code></a> on the buffer.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JamieMair/SimpleNNs.jl/blob/88e332cc9f5c7968a2e4023df140fc85272fdbb8/src/backprop/preallocation.jl#L7-L13">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="SimpleNNs.pullback!-Tuple{Any, Any, SimpleNNs.AbstractLayer}" href="#SimpleNNs.pullback!-Tuple{Any, Any, SimpleNNs.AbstractLayer}"><code>SimpleNNs.pullback!</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">pullback!(input_partials, output_partials, layer)</code></pre><p>Here, we complete the backpropagation of the partial gradients to the inputs of the current layer. This should be called after <code>backprop!</code>. This method will fill the <code>input_partials</code> buffer with partial gradients calculated via the chain rule from the gradients of the partials from this layer&#39;s output.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JamieMair/SimpleNNs.jl/blob/88e332cc9f5c7968a2e4023df140fc85272fdbb8/src/backprop/backprop.jl#L20-L27">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="SimpleNNs.relu-Tuple{Any}" href="#SimpleNNs.relu-Tuple{Any}"><code>SimpleNNs.relu</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">relu(x)</code></pre><p>Rectified linear unit activation function.</p><p>Computes max(0, x)</p><p><strong>Arguments</strong></p><p>x: Input value</p><p><strong>Returns</strong></p><p>Output in range (0, ∞)</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JamieMair/SimpleNNs.jl/blob/88e332cc9f5c7968a2e4023df140fc85272fdbb8/src/forward/activations.jl#L15-L26">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="SimpleNNs.remove_loss-Tuple{SimpleNNs.Model}" href="#SimpleNNs.remove_loss-Tuple{SimpleNNs.Model}"><code>SimpleNNs.remove_loss</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">remove_loss(model::Model)</code></pre><p>Create a new model with the loss layer removed from the end, if one exists.</p><p>This function reconstructs the model chain without the final loss layer. The original model&#39;s parameters are copied to the new model.</p><p><strong>Arguments</strong></p><ul><li><code>model::Model</code>: The model to remove the loss layer from</li></ul><p><strong>Returns</strong></p><p>A new <code>Model</code> without the loss layer. If the model doesn&#39;t have a loss layer, returns the original model unchanged.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">model_with_loss = chain(
    Static(10),
    Dense(5, activation_fn=identity),
    BatchCrossEntropyLoss(targets=zeros(Int, 32), num_classes=5)
)

model = remove_loss(model_with_loss)
has_loss(model)  # false</code></pre><p>See also <a href="#SimpleNNs.add_loss-Tuple{SimpleNNs.Model, SimpleNNs.AbstractTargetsLayer}"><code>add_loss</code></a>, <a href="#SimpleNNs.has_loss-Tuple{SimpleNNs.Model}"><code>has_loss</code></a>, <a href="#SimpleNNs.get_predictions-Tuple{SimpleNNs.Model, Any}"><code>get_predictions</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JamieMair/SimpleNNs.jl/blob/88e332cc9f5c7968a2e4023df140fc85272fdbb8/src/chain.jl#L274-L301">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="SimpleNNs.reset!-Tuple{SimpleNNs.AbstractOptimiser}" href="#SimpleNNs.reset!-Tuple{SimpleNNs.AbstractOptimiser}"><code>SimpleNNs.reset!</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">reset!(opt::AbstractOptimiser)</code></pre><p>Reset the internal state of the optimiser to its initial values.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JamieMair/SimpleNNs.jl/blob/88e332cc9f5c7968a2e4023df140fc85272fdbb8/src/optimisers/optimisers.jl#L6-L10">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="SimpleNNs.set_inputs!-Tuple{SimpleNNs.ForwardPassCache, Any}" href="#SimpleNNs.set_inputs!-Tuple{SimpleNNs.ForwardPassCache, Any}"><code>SimpleNNs.set_inputs!</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">set_inputs!(cache::ForwardPassCache, inputs)</code></pre><p>Sets the input array in the forward pass cache.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JamieMair/SimpleNNs.jl/blob/88e332cc9f5c7968a2e4023df140fc85272fdbb8/src/forward/preallocation.jl#L56-L60">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="SimpleNNs.sigmoid-Tuple{Any}" href="#SimpleNNs.sigmoid-Tuple{Any}"><code>SimpleNNs.sigmoid</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">sigmoid(x)</code></pre><p>Logistic sigmoid activation function.</p><p>Computes the sigmoid function: 1 / (1 + exp(-x)).</p><p><strong>Arguments</strong></p><p>x: Input value</p><p><strong>Returns</strong></p><p>Output in range (0, 1)</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JamieMair/SimpleNNs.jl/blob/88e332cc9f5c7968a2e4023df140fc85272fdbb8/src/forward/activations.jl#L2-L13">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="SimpleNNs.tanh_fast-Tuple{Float32}" href="#SimpleNNs.tanh_fast-Tuple{Float32}"><code>SimpleNNs.tanh_fast</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">tanh_fast(x)</code></pre><p>Fast hyperbolic tangent activation function.</p><p>Computes an optimized version of the hyperbolic tangent function. This may use approximations for better performance compared to the standard tan for Float32 and Float64.</p><p><strong>Arguments</strong></p><p>x: Input scalar</p><p><strong>Returns</strong></p><p>Output in range (-1, 1)</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JamieMair/SimpleNNs.jl/blob/88e332cc9f5c7968a2e4023df140fc85272fdbb8/src/forward/activations.jl#L30-L41">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="SimpleNNs.update!-Tuple{Any, Any, SimpleNNs.AbstractOptimiser}" href="#SimpleNNs.update!-Tuple{Any, Any, SimpleNNs.AbstractOptimiser}"><code>SimpleNNs.update!</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">update!(parameters, gradients, opt::AbstractOptimiser)</code></pre><p>Update the parameters using the provided gradients and optimiser.</p><p><strong>Arguments</strong></p><ul><li><code>parameters</code>: Model parameters to be updated</li><li><code>gradients</code>: Gradients computed from the loss function</li><li><code>opt</code> (AbstractOptimiser): The optimiser instance containing update rules</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JamieMair/SimpleNNs.jl/blob/88e332cc9f5c7968a2e4023df140fc85272fdbb8/src/optimisers/optimisers.jl#L15-L24">source</a></section></article></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../advanced_usage/">« Advanced Usage</a><a class="docs-footer-nextpage" href="../function_index/">Index »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.13.0 on <span class="colophon-date" title="Tuesday 17 February 2026 11:42">Tuesday 17 February 2026</span>. Using Julia version 1.11.9.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
