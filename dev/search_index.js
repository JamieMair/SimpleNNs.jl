var documenterSearchIndex = {"docs":
[{"location":"api/#API","page":"API","title":"API","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"Modules = [SimpleNNs, SimpleNNs.GPU]","category":"page"},{"location":"getting_started/#Getting-Started","page":"Getting Started","title":"Getting Started","text":"","category":"section"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"Firstly, you can add this package directly using the URL:","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"import Pkg; Pkg.add(\"https://github.com/JamieMair/SimpleNNs.jl\")","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"Note: This package has plans to be registered and should be available with ] add SimpleNNs in the future.","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"Once the package is installed, go ahead and load the package.","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"using SimpleNNs\nnothing # hide","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"To start with, let's create a simple test dataset:","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"batch_size = 256\nx = collect(LinRange(0, 2*pi, batch_size)')\ny = sin.(x)\nnothing # hide","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"Note that we use the adjoint ' so that the last dimension is the batch dimension.","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"We can now create our small neural network to fit a curve that maps from x to y. The syntax will be familiar to users of Flux.jl or SimpleChains.jl.","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"model = chain(\n    Static(1),\n    Dense(10, activation_fn=tanh),\n    Dense(10, activation_fn=sigmoid),\n    Dense(1, activation_fn=identity),\n);\nnothing # hide","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"Here, we specify the expected feature size of the input, leaving out the batch dimension.","category":"page"},{"location":"getting_started/#Inference-(Forward-Pass)","page":"Getting Started","title":"Inference (Forward-Pass)","text":"","category":"section"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"To run inference with this model, we first need to preallocate a buffer to store the intermediate forward pass values. This preallocation is by design, so that memory is only allocated once at the beginning of training.","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"forward_buffer = preallocate(model, batch_size);\nnothing # hide","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"This buffer also contains the input to the neural network. We can set the inputs to the neural network via","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"set_inputs!(forward_buffer, x);\nnothing # hide","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"The above function can be used to set the new inputs at each epoch.","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"We can access the flat parameter vector of the model via parameters(model) to initialise the weights of the network, i.e.","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"using Random\nRandom.seed!(1234)\nparams = parameters(model);\nrandn!(params);\nparams .*= 0.1;\nnothing # hide","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"We can run inference with","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"forward!(forward_buffer, model);\nyhat = get_outputs(forward_buffer);\nnothing # hide","category":"page"},{"location":"getting_started/#Training-(Backward-Pass)","page":"Getting Started","title":"Training (Backward-Pass)","text":"","category":"section"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"We can specify a mean-squared error loss via","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"loss = MSELoss(y);\nnothing # hide","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"and preallocate the buffer used for calculating the gradients via back-propagation:","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"gradient_buffer = preallocate_grads(model, batch_size);\nnothing # hide","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"Now we have all the ingredients we need to write a simple training script, making use of Optimisers.jl.","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"import Optimisers\n\nlr = 0.01\nopt = Optimisers.setup(Optimisers.Adam(lr), params)\nepochs = 1000\nlosses = zeros(Float32, epochs)\nfor i in 1:epochs\n    forward!(forward_buffer, model)\n    losses[i] = backprop!(gradient_buffer, forward_buffer, model, loss)\n    grads = gradients(gradient_buffer) # extract the gradient vector\n    # Apply the optimiser\n    Optimisers.update!(opt, params, grads)\nend\n\nnothing # hide","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"We can plot the losses over time, for example using Plots.jl:","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"using Plots\nusing PlotThemes # hide\ntheme(:dark) # hide\nplot(losses, xlabel=\"Epochs\", ylabel=\"MSE Loss\", lw=2, label=nothing)","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"Finally, we can run one final forward pass to get the predictions","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"forward!(forward_buffer, model);\nyhat = get_outputs(forward_buffer);\nnothing # hide","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"and then plot the predictions","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"using Plots\nplt = plot(x', y', linestyle=:solid, label=\"Original\", lw=2);\nplot!(plt, x', yhat', linestyle=:dashdot, label=\"Prediction\", lw=2);\nxlabel!(\"x\")\nylabel!(\"y\")\nplt","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"To see an example using convolution layers and GPU training, see the MNIST training example.","category":"page"},{"location":"function_index/#Index","page":"Index","title":"Index","text":"","category":"section"},{"location":"function_index/","page":"Index","title":"Index","text":"","category":"page"},{"location":"","page":"Home","title":"Home","text":"CurrentModule = SimpleNNs","category":"page"},{"location":"#SimpleNNs","page":"Home","title":"SimpleNNs","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"SimpleNNs.jl is heavily inspired by SimpleChains.jl, which showed that there is space for micro-optimisations to be very important for small neural networks (see the blog post). This project aims to expand upon SimpleChains.jl by introducing both CPU and GPU support.","category":"page"},{"location":"","page":"Home","title":"Home","text":"As the name suggests, this is not a fully featured neural network library, and most notably, it does not include auto-differentiation capabilities. The goals of this package are the following:","category":"page"},{"location":"","page":"Home","title":"Home","text":"To build simple neural network architectures, whose parameters are represented as a simple flat vector.\nTo be able to train and run these neural networks with pre-allocated buffers to avoid memory allocations.\nTo be executable on either the GPU or the CPU.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Currently, there is full support for dense layers with the traditional activation functions: textReLU, tanh (hyperbolic tangent) and sigma (logistic sigmoid). Custom loss functions will work on forward passes, but the gradient must be overloaded as detailed in another page of this documentation. Convolutional layers are also supported, with some limitations on parameters such as stride.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Head over to the Getting Started page to see how to use this package.","category":"page"},{"location":"#Documentation-Outline","page":"Home","title":"Documentation Outline","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Pages = [\n    \"getting_started.md\",\n    \"mnist.md\",\n    \"api.md\",\n    \"function_index.md\"\n]\nDepth = 2","category":"page"},{"location":"mnist/#MNIST","page":"MNIST","title":"MNIST","text":"","category":"section"},{"location":"mnist/","page":"MNIST","title":"MNIST","text":"In this example, we will train a small neural network to classify the MNIST digit dataset. We will use MLDatasets to load our dataset.","category":"page"},{"location":"mnist/","page":"MNIST","title":"MNIST","text":"using MLDatasets\ndataset = MNIST(:train);\nimages, labels = dataset[:];\n# reshape images array to add in a channel\nimages = reshape(images, size(images, 1), size(images, 2), 1, size(images, 3));\n# Make our labels from 1 to 10 instead\nlabels .+= 1;\n","category":"page"},{"location":"mnist/","page":"MNIST","title":"MNIST","text":"If you have an NVIDIA GPU, you can modify the code below to set use_gpu equal to true.","category":"page"},{"location":"mnist/","page":"MNIST","title":"MNIST","text":"using SimpleNNs\nimport SimpleNNs.GPU: gpu\nuse_gpu = true;\nto_device = use_gpu ? gpu : identity;\n","category":"page"},{"location":"mnist/","page":"MNIST","title":"MNIST","text":"We can use this function to put our data onto the GPU, using the pipe operator:","category":"page"},{"location":"mnist/","page":"MNIST","title":"MNIST","text":"images = images |> to_device;\nlabels = labels |> to_device;\n","category":"page"},{"location":"mnist/","page":"MNIST","title":"MNIST","text":"Next, we can create our model:","category":"page"},{"location":"mnist/","page":"MNIST","title":"MNIST","text":"img_size = size(images)[1:end-1]\nmodel = chain(\n    Static(img_size),\n    Conv((5,5), 16; activation_fn=relu),\n    MaxPool((2,2)),\n    Conv((3,3), 8; activation_fn=relu),\n    MaxPool((4,4)),\n    Flatten(),\n    Dense(10, activation_fn=identity)\n) |> to_device;\n","category":"page"},{"location":"mnist/","page":"MNIST","title":"MNIST","text":"For training, we will use stochastic gradient descent (with the ADAM optimiser), with a batch size of 32. We need to preallocate our buffers, as below:","category":"page"},{"location":"mnist/","page":"MNIST","title":"MNIST","text":"batch_size = 32;\nforward_buffer = preallocate(model, batch_size);\ngradient_buffer = preallocate_grads(model, batch_size);\n","category":"page"},{"location":"mnist/","page":"MNIST","title":"MNIST","text":"Now, we write our training loop:","category":"page"},{"location":"mnist/","page":"MNIST","title":"MNIST","text":"using Random\nRandom.seed!(1234)\nparams = parameters(model);\nrandn!(params)\nparams .*= 0.05\nimport Optimisers\nlr = 0.01;\nopt = Optimisers.setup(Optimisers.Adam(lr), params);\nepochs = 1000;\nlosses = zeros(Float32, epochs);\ntraining_indices = collect(1:length(labels));\n\nfor i in 1:epochs\n    # Select a random batch\n    Random.shuffle!(training_indices)\n    batch_indices = view(training_indices, 1:batch_size)\n    # Set the inputs of the forward buffer to this minibatch\n    set_inputs!(forward_buffer, view(images, :, :, :, batch_indices));\n    # Create a loss function that wraps the current minibatch labels\n    loss = LogitCrossEntropyLoss(view(labels, batch_indices), 10);\n\n\n    forward!(forward_buffer, model)\n    losses[i] = backprop!(gradient_buffer, forward_buffer, model, loss)\n    grads = gradients(gradient_buffer) # extract the gradient vector\n    # Apply the optimiser\n    Optimisers.update!(opt, params, grads)\nend\n\n","category":"page"},{"location":"mnist/","page":"MNIST","title":"MNIST","text":"We can plot the losses over time, for example using Plots.jl:","category":"page"},{"location":"mnist/","page":"MNIST","title":"MNIST","text":"using Plots\nusing PlotThemes # hide\ntheme(:dark) # hide\nplot(losses, xlabel=\"Epochs\", ylabel=\"Cross Entropy Loss\", lw=2, label=nothing)","category":"page"},{"location":"mnist/","page":"MNIST","title":"MNIST","text":"Finally, we can test the accuracy of the model by loading the test set:","category":"page"},{"location":"mnist/","page":"MNIST","title":"MNIST","text":"dataset = MNIST(:test);\ntest_images, test_labels = dataset[:];\n# reshape images array to add in a channel\ntest_images = reshape(test_images, size(test_images, 1), size(test_images, 2), 1, size(test_images, 3));\n# Make our labels from 1 to 10 instead\ntest_labels .+= 1;\ntest_images = test_images |> to_device;\ntest_labels = test_labels |> to_device;\n","category":"page"},{"location":"mnist/","page":"MNIST","title":"MNIST","text":"Now, we create a forward buffer for the test images:","category":"page"},{"location":"mnist/","page":"MNIST","title":"MNIST","text":"test_forward_buffer = preallocate(model, length(test_labels));\nset_inputs!(test_forward_buffer, test_images);\nforward!(test_forward_buffer, model);\nlogits = get_outputs(test_forward_buffer);\npredictions = reshape([i[1] for i in Array(argmax(logits, dims=1))], :) |> to_device;\naccuracy = sum(predictions .== test_labels) / length(test_labels) * 100\n@show accuracy","category":"page"}]
}
