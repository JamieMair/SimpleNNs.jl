var documenterSearchIndex = {"docs":
[{"location":"advanced_usage/#Advanced-Usage","page":"Advanced Usage","title":"Advanced Usage","text":"","category":"section"},{"location":"advanced_usage/","page":"Advanced Usage","title":"Advanced Usage","text":"This section covers advanced topics and optimisation techniques for SimpleNNs.jl.","category":"page"},{"location":"advanced_usage/#Custom-Training-Loops","page":"Advanced Usage","title":"Custom Training Loops","text":"","category":"section"},{"location":"advanced_usage/#Function-Approximation","page":"Advanced Usage","title":"Function Approximation","text":"","category":"section"},{"location":"advanced_usage/","page":"Advanced Usage","title":"Advanced Usage","text":"Here's a simple example of fitting a non-linear function, using a custom training loop and optimiser (in this case a custom implementation of ADAM):","category":"page"},{"location":"advanced_usage/","page":"Advanced Usage","title":"Advanced Usage","text":"using SimpleNNs\nusing Random\nusing ProgressBars\n\n# Define a non-linear target function\nfunction target_function(x)\n    freq = 4.0\n    offset = 1.0\n    return x * sin(x * freq - offset) + offset\nend\n\n# Create training data\nN = 512\ninputs = Float32.(reshape(collect(LinRange(-1.0, 1.0, N)), 1, N))\noutputs = Float32.(reshape(target_function.(inputs), 1, :))\n\n# Create a deeper network\nmodel = chain(\n    Static(1),\n    Dense(16, activation_fn=tanh_fast),\n    Dense(16, activation_fn=tanh_fast),\n    Dense(16, activation_fn=tanh_fast),\n    Dense(1, activation_fn=identity)\n)\n\n# Initialise parameters\nRandom.seed!(42)\nps = SimpleNNs.parameters(model)\nrandn!(ps)\nps .*= 0.1f0\n\n# Preallocate\nforward_cache = preallocate(model, N)\nbackward_cache = preallocate_grads(model, N)\nset_inputs!(forward_cache, inputs)\n\n# Loss function\nloss = MSELoss(outputs)\n\n# ADAM optimiser parameters\nepochs = 5000\nadam_opt = AdamOptimiser(backward_cache.parameter_gradients; lr=0.02f0/N, beta_1=0.9f0, beta_2=0.999f0)\n\nlosses = Float32[]\n\nfor epoch in ProgressBar(1:epochs)\n    # Forward pass\n    forward!(forward_cache, model)\n    \n    # Compute loss\n    current_loss = backprop!(backward_cache, forward_cache, model, loss)\n    push!(losses, current_loss)\n    \n    # Apply optimiser update\n    grads = gradients(backward_cache)\n    update!(ps, grads, adam_opt)\nend\n\n# Final prediction\nforward!(forward_cache, model)\npredictions = get_outputs(forward_cache)\n\nprintln(\"Final loss: \", losses[end])","category":"page"},{"location":"advanced_usage/#Batch-Processing-and-Data-Loading","page":"Advanced Usage","title":"Batch Processing and Data Loading","text":"","category":"section"},{"location":"advanced_usage/","page":"Advanced Usage","title":"Advanced Usage","text":"For larger datasets, you'll need efficient batch processing:","category":"page"},{"location":"advanced_usage/","page":"Advanced Usage","title":"Advanced Usage","text":"using SimpleNNs\nusing MLDatasets\nusing Random\nusing ProgressBars\n\nstruct DataLoader{T,S}\n    features::T\n    labels::S\n    batch_size::Int\n    indices::Vector{Int}\n    n_samples::Int\nend\n\nfunction DataLoader(features, labels, batch_size::Int)\n    n_samples = size(features)[end]\n    indices = collect(1:batch_size)\n    return DataLoader(features, labels, batch_size, indices, n_samples)\nend\n\nfunction next_batch!(loader::DataLoader)\n    # Randomly sample new indices\n    shuffle!(loader.indices)\n    return (\n        view(loader.features, ntuple(i -> :, ndims(loader.features)-1)..., loader.indices),\n        view(loader.labels, loader.indices)\n    )\nend\n\n# Example with MNIST\ndataset = MNIST(:train)\nimages, labels = dataset[:]\nimages = reshape(images, 28, 28, 1, size(images, 3))\nlabels = labels .+ 1  # 1-indexed\n\n# Create data loader\nbatch_size = 128\nloader = DataLoader(images, labels, batch_size)\n\n# Create model\nmodel = chain(\n    Static((28, 28, 1)),\n    Conv((5,5), 16, activation_fn=relu),\n    MaxPool((2,2)),\n    Conv((3,3), 8, activation_fn=relu),\n    MaxPool((4,4)),\n    Flatten(),\n    Dense(10, activation_fn=identity)\n)\n\n# Preallocate\nforward_cache = preallocate(model, batch_size)\nbackward_cache = preallocate_grads(model, batch_size)\n\n# Training loop with batch processing\nfor epoch in ProgressBar(1:100)\n    batch_features, batch_labels = next_batch!(loader)\n    \n    set_inputs!(forward_cache, batch_features)\n    loss = LogitCrossEntropyLoss(batch_labels, 10)\n    \n    forward!(forward_cache, model)\n    total_loss = backprop!(backward_cache, forward_cache, model, loss)\n    \n    # Apply gradients (simplified SGD)\n    ps = SimpleNNs.parameters(model)\n    grads = gradients(backward_cache)\n    ps .-= 0.001f0 .* grads\nend","category":"page"},{"location":"advanced_usage/#Memory-Optimisation","page":"Advanced Usage","title":"Memory Optimisation","text":"","category":"section"},{"location":"advanced_usage/#Cache-Truncation","page":"Advanced Usage","title":"Cache Truncation","text":"","category":"section"},{"location":"advanced_usage/","page":"Advanced Usage","title":"Advanced Usage","text":"When your actual batch size is smaller than your preallocated cache, use truncation:","category":"page"},{"location":"advanced_usage/","page":"Advanced Usage","title":"Advanced Usage","text":"# Preallocate for maximum batch size\nmax_batch_size = 256\nforward_cache = preallocate(model, max_batch_size)\nbackward_cache = preallocate_grads(model, max_batch_size)\n\n# For smaller batches, truncate the cache\nactual_batch_size = 64\ntruncated_forward = SimpleNNs.truncate(forward_cache, actual_batch_size)\ntruncated_backward = SimpleNNs.truncate(backward_cache, actual_batch_size)\n\n# Use truncated caches\nset_inputs!(truncated_forward, small_batch_data)\nforward!(truncated_forward, model)\nbackprop!(truncated_backward, truncated_forward, model, loss)","category":"page"},{"location":"advanced_usage/#Performance-Profiling","page":"Advanced Usage","title":"Performance Profiling","text":"","category":"section"},{"location":"advanced_usage/#Allocation-Tracking","page":"Advanced Usage","title":"Allocation Tracking","text":"","category":"section"},{"location":"advanced_usage/","page":"Advanced Usage","title":"Advanced Usage","text":"SimpleNNs.jl is designed for zero-allocation (or close to zero) inference. Some minor allocations may slip through, depending on the version of Julia you are currently running. An example of testing the allocations is below:","category":"page"},{"location":"advanced_usage/","page":"Advanced Usage","title":"Advanced Usage","text":"using SimpleNNs\n\nmodel = chain(Static(10), Dense(32, activation_fn=relu), Dense(1))\nforward_cache = preallocate(model, 64)\nbackward_cache = preallocate_grads(model, 64)\n\ninputs = randn(Float32, 10, 64)\ntargets = randn(Float32, 1, 64)\nloss = MSELoss(targets)\n\nset_inputs!(forward_cache, inputs)\n\n# Call once to remove allocations from JIT compilation\nforward!(forward_cache, model)\n# Check allocations in forward pass\nforward_allocs = @allocations forward!(forward_cache, model)\nprintln(\"Forward allocations: $forward_allocs\")\n\n# Again, call once to avoid measuring JIT\nbackprop!(backward_cache, forward_cache, model, loss)\n# Check allocations in backward pass\nbackward_allocs = @allocations backprop!(backward_cache, forward_cache, model, loss)\nprintln(\"Backward allocations: $backward_allocs\")\n\n# Should be 0 or very low for optimal performance\n@assert forward_allocs <= 1\n@assert backward_allocs <= 1","category":"page"},{"location":"advanced_usage/#Custom-Activation-Functions","page":"Advanced Usage","title":"Custom Activation Functions","text":"","category":"section"},{"location":"advanced_usage/","page":"Advanced Usage","title":"Advanced Usage","text":"While SimpleNNs.jl has built-in activation functions, you can define custom ones:","category":"page"},{"location":"advanced_usage/","page":"Advanced Usage","title":"Advanced Usage","text":"# Custom activation function\nfunction swish(x)\n    return x * sigmoid(x)\nend\n\n# Custom gradient (if needed for backward pass) (input is the output of the activation)\nfunction swish_gradient(x)\n    s = sigmoid(x)\n    return s * (1 + x * (1 - s))\nend\n# Link the gradient fn to the activation fn\nSimpleNNs.activation_gradient_fn(::typeof(swish)) = swish_gradient\n\nmodel = chain(\n    Static(10),\n    Dense(32, activation_fn=swish),  # Custom activation\n    Dense(1, activation_fn=identity)\n)\n\n# ... Model should now work with forward! and backprop!","category":"page"},{"location":"advanced_usage/","page":"Advanced Usage","title":"Advanced Usage","text":"warning: Custom Activations\nCustom activation functions work for forward passes, but you may need to implement custom backward pass logic for training.","category":"page"},{"location":"advanced_usage/#Model-Serialisation","page":"Advanced Usage","title":"Model Serialisation","text":"","category":"section"},{"location":"advanced_usage/","page":"Advanced Usage","title":"Advanced Usage","text":"Save and load trained models:","category":"page"},{"location":"advanced_usage/","page":"Advanced Usage","title":"Advanced Usage","text":"using JLD2\n\n# Save model parameters\n@save \"model_params.jld2\" params=parameters(model)\n\n# Load model parameters\n@load \"model_params.jld2\" params\nnew_model = create_model()  # Recreate same architecture\nparameters(new_model) .= params","category":"page"},{"location":"advanced_usage/","page":"Advanced Usage","title":"Advanced Usage","text":"This advanced usage guide should help you get the most out of SimpleNNs.jl for complex applications and high-performance computing scenarios.","category":"page"},{"location":"advanced_usage/#Optimiser-Comparison","page":"Advanced Usage","title":"Optimiser Comparison","text":"","category":"section"},{"location":"advanced_usage/","page":"Advanced Usage","title":"Advanced Usage","text":"You can easily compare different optimisers:","category":"page"},{"location":"advanced_usage/","page":"Advanced Usage","title":"Advanced Usage","text":"using SimpleNNs\nusing Random\n\n# Create test problem\nmodel = chain(Static(2), Dense(10, activation_fn=tanh), Dense(1))\nbatch_size = 32\nforward_cache = preallocate(model, batch_size)\nbackward_cache = preallocate_grads(model, batch_size)\n\ninputs = randn(Float32, 2, batch_size)\ntargets = randn(Float32, 1, batch_size)\nset_inputs!(forward_cache, inputs)\nloss = MSELoss(targets)\n\n# Test different optimisers\noptimisers = [\n    (\"SGD\", SGDOptimiser(backward_cache.parameter_gradients; lr=0.01f0)),\n    (\"SGD+Momentum\", SGDOptimiser(backward_cache.parameter_gradients; lr=0.01f0, momentum=0.9f0)),\n    (\"RMSProp\", RMSPropOptimiser(backward_cache.parameter_gradients; lr=0.001f0)),\n    (\"Adam\", AdamOptimiser(backward_cache.parameter_gradients; lr=0.001f0))\n]\n\nfor (name, opt) in optimisers\n    # Reset model parameters\n    Random.seed!(42)\n    params = parameters(model)\n    randn!(params)\n    params .*= 0.1f0\n    reset!(opt)\n    \n    println(\"Training with $name:\")\n    for epoch in 1:100\n        forward!(forward_cache, model)\n        current_loss = backprop!(backward_cache, forward_cache, model, loss)\n        \n        grads = gradients(backward_cache)\n        update!(params, grads, opt)\n        \n        if epoch % 25 == 0\n            println(\"  Epoch $epoch: Loss = $(round(current_loss, digits=6))\")\n        end\n    end\n    println()\nend","category":"page"},{"location":"api/#API","page":"API","title":"API","text":"","category":"section"},{"location":"api/#SimpleNNs.AdamOptimiser-Union{Tuple{AbstractArray{T}}, Tuple{T}} where T","page":"API","title":"SimpleNNs.AdamOptimiser","text":"AdamOptimiser(gradients::AbstractArray{T}; lr = Float32(1e-3), beta_1 = 0.9f0, beta_2 = 0.999f0) where {T}\n\nCreate an Adam optimiser for gradient-based parameter updates.\n\nArguments\n\ngradients (AbstractArray{T}): Template array matching the shape of gradients to be optimised\n\nKeyword Arguments\n\nlr (T): Learning rate (default: 1e-3)\nbeta_1 (T): Exponential decay rate for first moment estimates (default: 0.9)\nbeta_2 (T): Exponential decay rate for second moment estimates (default: 0.999)\n\n\n\n\n\n","category":"method"},{"location":"api/#SimpleNNs.Conv-Union{Tuple{N}, Tuple{NTuple{N, Int64}, Int64}} where N","page":"API","title":"SimpleNNs.Conv","text":"Conv(kernel_size::NTuple{N, Int}, out_channels::Int; kwargs...)\n\nA convolutional layer with a given kernel size and specified number of output channels.\n\nThis can automatically infer the number of input channels based on the preceeding layers.\n\nKeyword Arguments\n\nuse_bias (default: Val(true)) - Whether or not to add a bias vector to the output. Wrapped in a Val for optimisation.\nactivation_fn (default: identity) - A custom activation function. Note that not all functions are supported by backpropagation.\nparameter_type (default: Val(Float32)) - The datatype to use for the parameters, wrapped in a Val type.\n\n\n\n\n\n","category":"method"},{"location":"api/#SimpleNNs.Dense-Tuple{Integer}","page":"API","title":"SimpleNNs.Dense","text":"Dense(outputs::Integer; kwargs...)\n\nA representation of a dense layer. By default this can be constructed by  specifying the desired number of outputs. The input size can be inferred from the rest of the chain when constructing a model.\n\nKeyword Arguments\n\nuse_bias (default: Val(true)) - Whether or not to add a bias vector to the output. Wrapped in a Val for optimisation.\nactivation_fn (default: identity) - A custom activation function. Note that not all functions are supported by backpropagation.\nparameter_type (default: Val(Float32)) - The datatype to use for the parameters, wrapped in a Val type.\ninputs (default: Infer()) - Specify the number of inputs, or infer them from the rest of the model.\n\n\n\n\n\n","category":"method"},{"location":"api/#SimpleNNs.Flatten","page":"API","title":"SimpleNNs.Flatten","text":"Flatten()\n\nFlatten the dimensions of the preceeding layer, leaving the batch dimension unaffected. The output should be (k x n) where k is the product of the non-batch dimensions of the previous layer.\n\n\n\n\n\n","category":"type"},{"location":"api/#SimpleNNs.LogitCrossEntropyLoss","page":"API","title":"SimpleNNs.LogitCrossEntropyLoss","text":"LogitCrossEntropyLoss(targets, num_classes::Int)\n\nExpects the targets in a single vector containg class labels, which have to be between 1 and num_classes inclusive.\n\n\n\n\n\n","category":"type"},{"location":"api/#SimpleNNs.MSELoss","page":"API","title":"SimpleNNs.MSELoss","text":"MSELoss(targets)\n\nExpects the targets in the form (K x N) where K is the output dimension (usually 1) and N is the batch size.\n\nFor efficiency, this is just ∑ (y-̂y)² and NOT scaled by a half.\n\n\n\n\n\n","category":"type"},{"location":"api/#SimpleNNs.MaxPool-Union{Tuple{NTuple{N, Int64}}, Tuple{N}} where N","page":"API","title":"SimpleNNs.MaxPool","text":"MaxPool(pool_size::NTuple{N, Int}; kwargs...)\n\nA convolutional max-pool layer with a given kernel size.\n\nThis can automatically infer the necessary sizes if specified.\n\n\n\n\n\n","category":"method"},{"location":"api/#SimpleNNs.RMSPropOptimiser","page":"API","title":"SimpleNNs.RMSPropOptimiser","text":"RMSPropOptimiser{T, X<:AbstractArray{T}} <: AbstractOptimiser\n\nRMSProp optimiser with exponential moving average of squared gradients.\n\nFields\n\nlr::T: Learning rate\nrho::T: Exponential decay rate for moving average\neps::T: Small constant for numerical stability\nv::X: Exponential moving average of squared gradients\n\n\n\n\n\n","category":"type"},{"location":"api/#SimpleNNs.RMSPropOptimiser-Union{Tuple{AbstractArray{T}}, Tuple{T}} where T","page":"API","title":"SimpleNNs.RMSPropOptimiser","text":"RMSPropOptimiser(gradients::AbstractArray{T}; lr = Float32(1e-3), rho = 0.9f0, eps = Float32(1e-8)) where {T}\n\nCreate an RMSProp optimiser for gradient-based parameter updates.\n\nRMSProp maintains a moving average of squared gradients to adaptively scale the learning rate.\n\nArguments\n\ngradients (AbstractArray{T}): Template array matching the shape of gradients to be optimised\n\nKeyword Arguments\n\nlr (T): Learning rate (default: 1e-3)\nrho (T): Exponential decay rate for moving average of squared gradients (default: 0.9)\neps (T): Small constant added to denominator for numerical stability (default: 1e-8)\n\nExamples\n\nopt = RMSPropOptimiser(gradients; lr=0.001f0, rho=0.9f0)\n\n\n\n\n\n","category":"method"},{"location":"api/#SimpleNNs.SGDOptimiser","page":"API","title":"SimpleNNs.SGDOptimiser","text":"SGDOptimiser{T} <: AbstractOptimiser\n\nStochastic Gradient Descent optimiser with optional momentum.\n\nFields\n\nlr::T: Learning rate\nmomentum::T: Momentum coefficient (0.0 for no momentum)\nvelocity::AbstractArray{T}: Velocity buffer for momentum (internal state)\n\n\n\n\n\n","category":"type"},{"location":"api/#SimpleNNs.SGDOptimiser-Union{Tuple{AbstractArray{T}}, Tuple{T}} where T","page":"API","title":"SimpleNNs.SGDOptimiser","text":"SGDOptimiser(gradients::AbstractArray{T}; lr = Float32(1e-3), momentum = 0.0f0) where {T}\n\nCreate an SGD optimiser for gradient-based parameter updates.\n\nArguments\n\ngradients (AbstractArray{T}): Template array matching the shape of gradients to be optimised\n\nKeyword Arguments\n\nlr (T): Learning rate (default: 1e-3)\nmomentum (T): Momentum coefficient, 0.0 for standard SGD (default: 0.0)\n\nExamples\n\n# Standard SGD\nopt = SGDOptimiser(gradients; lr=0.01f0)\n\n# SGD with momentum\nopt = SGDOptimiser(gradients; lr=0.01f0, momentum=0.9f0)\n\n\n\n\n\n","category":"method"},{"location":"api/#SimpleNNs.Static-Tuple{Union{Int64, NTuple{N, T} where {N, T}}}","page":"API","title":"SimpleNNs.Static","text":"Static(inputs::Union{Int, NTuple}; kwargs...)\n\nUsed for specifying the input type to a neural network. inputs should  be a single integer for a dense network, representing the number of  features. For a image network, inputs can be a tuple specifying the size of the images in the form (WIDTH, HEIGHT, CHANNELS).\n\n\n\n\n\n","category":"method"},{"location":"api/#SimpleNNs.activation_gradient_fn-Tuple{Any}","page":"API","title":"SimpleNNs.activation_gradient_fn","text":"Dertivatives are used to backpropagate the gradients of the layer outputs back to the activations of that layer. To save space, these are calculated exclusively using the outputs of the layer. Instead of functions written as dy/dx=f(x), we  instead write dy/dx = g(y). This can be done for the 3 major functions.\n\nWhenever y is used below, assume this is a function of the output, not the input.\n\n\n\n\n\n","category":"method"},{"location":"api/#SimpleNNs.backprop!-NTuple{5, Any}","page":"API","title":"SimpleNNs.backprop!","text":"backprop!(partials_buffer, gradient_buffer, inputs, outputs, layer)\n\nBackpropagates the partial gradients of the outputs of the current layer into the parameters of the current layer. partial_buffers is used as a buffer for the gradients of the output of this layer. gradient_buffer should be  filled up with the gradients of the parameters of the current layer, using the chain rule. inputs is the array fed into the layer and outputs is the output of this layer in the forward pass. layer is the struct containing information about the layer.\n\n\n\n\n\n","category":"method"},{"location":"api/#SimpleNNs.chain-Tuple","page":"API","title":"SimpleNNs.chain","text":"chain(layers...)\n\nCombines the given layer definitions into a single model and propagates the layer sizes through the network.\n\nThe first layer must always be a Static layer which specifies the feature size. If this is a simple fully connection network, then the first layer should be Static(nf) where nf is the number of features in your input matrix. Do not specify the batch size in this static input.\n\nThe default datatype for most layers is Float32, but this may be changed. The parameters of the entire model must be of the same datatype. This function will create a flat parameter vector for the model which can be accessed using the parameters function.\n\nExamples\n\nA simple dense, fully-connected, neural network which has 3 input features:\n\nmodel = chain(\n    Static(3),\n    Dense(10, activation_fn=tanh),\n    Dense(10, activation_fn=sigmoid),\n    Dense(1, activation_fn=identity),\n);\n\nAn example convolutional neural network:\n\n# Image size is (WIDTH, HEIGHT, CHANNELS)\nimg_size = (28, 28, 1)\nmodel = chain(\n    Static(img_size),\n    Conv((5,5), 16; activation_fn=relu),\n    MaxPool((2,2)),\n    Conv((3,3), 8; activation_fn=relu),\n    MaxPool((4,4)),\n    Flatten(),\n    Dense(10, activation_fn=identity)\n)\n\nSee also Static, Dense, Conv, MaxPool, Flatten and preallocate.\n\n\n\n\n\n","category":"method"},{"location":"api/#SimpleNNs.forward!-Tuple{SimpleNNs.ForwardPassCache, SimpleNNs.Model}","page":"API","title":"SimpleNNs.forward!","text":"forward!(cache::ForwardPassCache, model::Model)\n\nExecute a forward pass through the neural network model.\n\nThis function computes the forward propagation through all layers of the model, storing intermediate results in the pre-allocated cache. This is a zero-allocation operation when used with properly pre-allocated caches.\n\nArguments\n\ncache::ForwardPassCache: Pre-allocated cache containing input data and space for intermediate results\nmodel::Model: The neural network model to evaluate\n\nReturns\n\nThe cache object (for convenience), with updated intermediate and output values\n\nExamples\n\n# Create model and data\nmodel = chain(Static(4), Dense(8, activation_fn=relu), Dense(1))\ninputs = randn(Float32, 4, 32)  # 32 samples, 4 features each\n\n# Pre-allocate cache and set inputs\ncache = preallocate(model, 32)\nset_inputs!(cache, inputs)\n\n# Execute forward pass\nforward!(cache, model)\n\n# Get outputs\noutputs = get_outputs(cache)\n\nNotes\n\nRequires pre-allocated cache from preallocate(model, batch_size)\nInput data must be set using set_inputs!(cache, inputs) before calling\nThis is a mutating operation that modifies the cache in-place\nDesigned for zero allocations when properly used\nWorks on both CPU and GPU when model and data are on the same device\n\nSee also: preallocate, set_inputs!, get_outputs\n\n\n\n\n\n","category":"method"},{"location":"api/#SimpleNNs.get_outputs-Tuple{SimpleNNs.ForwardPassCache}","page":"API","title":"SimpleNNs.get_outputs","text":"get_outputs(cache::ForwardPassCache)\n\nGets the last output from the forward pass buffer.\n\n\n\n\n\n","category":"method"},{"location":"api/#SimpleNNs.gpu-Tuple{Any}","page":"API","title":"SimpleNNs.gpu","text":"gpu(x)\n\nMove data or models to GPU using CUDA. This function requires CUDA.jl, cuDNN.jl, and NNlib.jl  to be loaded before use.\n\nArguments\n\nx: The object to move to GPU. Can be a Model, AbstractArray, or other supported types.\n\nReturns\n\nGPU version of the input object\n\nExamples\n\nusing CUDA, cuDNN, NNlib\nusing SimpleNNs\n\n# Move model to GPU\nmodel = chain(Static(10), Dense(5))\ngpu_model = gpu(model)\n\n# Move array to GPU  \ncpu_array = randn(Float32, 10, 32)\ngpu_array = gpu(cpu_array)\n\nNotes\n\nRequires NVIDIA GPU with CUDA support\nCUDA.jl, cuDNN.jl, and NNlib.jl must be loaded before calling this function\nFor models, creates a new model with parameters on GPU\nFor arrays, converts to CuArray\nReturns input unchanged with warning for unsupported types\n\n\n\n\n\n","category":"method"},{"location":"api/#SimpleNNs.gradients-Tuple{SimpleNNs.BackpropagationCache}","page":"API","title":"SimpleNNs.gradients","text":"gradients(cache::BackpropagationCache)\n\nExtracts the gradient array from the backwards pass buffer, filled from use of the backprop! function.\n\n\n\n\n\n","category":"method"},{"location":"api/#SimpleNNs.parameters-Tuple{SimpleNNs.Model}","page":"API","title":"SimpleNNs.parameters","text":"parameters(model::Model)\n\nReturns the array used to store the parameters of the model.\n\nModifying this array will change the parameters of the model.\n\n\n\n\n\n","category":"method"},{"location":"api/#SimpleNNs.preallocate-Tuple{SimpleNNs.Model, Integer}","page":"API","title":"SimpleNNs.preallocate","text":"preallocate(model::Model, batch_size::Integer)\n\nCreates a buffer to store the intermediate layer outputs of a forward pass, along with the input.\n\nThe inputs can be set using set_inputs! and the outputs can be retrieved using get_outputs.\n\n\n\n\n\n","category":"method"},{"location":"api/#SimpleNNs.preallocate_grads-Tuple{SimpleNNs.Model, Integer}","page":"API","title":"SimpleNNs.preallocate_grads","text":"preallocategrads(model::Model, batchsize::Integer)\n\nCreates a buffer to store the intermediate arrays needed for backpropagation.\n\nThe gradients can be retrieved from the buffer using gradients on the buffer.\n\n\n\n\n\n","category":"method"},{"location":"api/#SimpleNNs.pullback!-Tuple{Any, Any, SimpleNNs.AbstractLayer}","page":"API","title":"SimpleNNs.pullback!","text":"pullback!(input_partials, output_partials, layer)\n\nHere, we complete the backpropagation of the partial gradients to the inputs of the current layer. This should be called after backprop!. This method will fill the input_partials buffer with partial gradients calculated via the chain rule from the gradients of the partials from this layer's output.\n\n\n\n\n\n","category":"method"},{"location":"api/#SimpleNNs.relu-Tuple{Any}","page":"API","title":"SimpleNNs.relu","text":"relu(x)\n\nRectified linear unit activation function.\n\nComputes max(0, x)\n\nArguments\n\nx: Input value\n\nReturns\n\nOutput in range (0, ∞)\n\n\n\n\n\n","category":"method"},{"location":"api/#SimpleNNs.reset!-Tuple{SimpleNNs.AbstractOptimiser}","page":"API","title":"SimpleNNs.reset!","text":"reset!(opt::AbstractOptimiser)\n\nReset the internal state of the optimiser to its initial values.\n\n\n\n\n\n","category":"method"},{"location":"api/#SimpleNNs.set_inputs!-Tuple{SimpleNNs.ForwardPassCache, Any}","page":"API","title":"SimpleNNs.set_inputs!","text":"set_inputs!(cache::ForwardPassCache, inputs)\n\nSets the input array in the forward pass cache.\n\n\n\n\n\n","category":"method"},{"location":"api/#SimpleNNs.sigmoid-Tuple{Any}","page":"API","title":"SimpleNNs.sigmoid","text":"sigmoid(x)\n\nLogistic sigmoid activation function.\n\nComputes the sigmoid function: 1 / (1 + exp(-x)).\n\nArguments\n\nx: Input value\n\nReturns\n\nOutput in range (0, 1)\n\n\n\n\n\n","category":"method"},{"location":"api/#SimpleNNs.tanh_fast-Tuple{Float32}","page":"API","title":"SimpleNNs.tanh_fast","text":"tanh_fast(x)\n\nFast hyperbolic tangent activation function.\n\nComputes an optimized version of the hyperbolic tangent function. This may use approximations for better performance compared to the standard tan for Float32 and Float64.\n\nArguments\n\nx: Input scalar\n\nReturns\n\nOutput in range (-1, 1)\n\n\n\n\n\n","category":"method"},{"location":"api/#SimpleNNs.update!-Tuple{Any, Any, SimpleNNs.AbstractOptimiser}","page":"API","title":"SimpleNNs.update!","text":"update!(parameters, gradients, opt::AbstractOptimiser)\n\nUpdate the parameters using the provided gradients and optimiser.\n\nArguments\n\nparameters: Model parameters to be updated\ngradients: Gradients computed from the loss function\nopt (AbstractOptimiser): The optimiser instance containing update rules\n\n\n\n\n\n","category":"method"},{"location":"getting_started/#Getting-Started","page":"Getting Started","title":"Getting Started","text":"","category":"section"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"Firstly, you can add this package directly using the URL:","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"import Pkg; Pkg.add(\"https://github.com/JamieMair/SimpleNNs.jl\")","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"Note: This package has plans to be registered and should be available with ] add SimpleNNs in the future.","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"Once the package is installed, go ahead and load the package.","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"using SimpleNNs\nnothing # hide","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"To start with, let's create a simple test dataset:","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"batch_size = 256\nx = collect(LinRange(0, 2*pi, batch_size)')\ny = sin.(x)\nnothing # hide","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"Note that we use the adjoint ' so that the last dimension is the batch dimension.","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"We can now create our small neural network to fit a curve that maps from x to y. The syntax will be familiar to users of Flux.jl or SimpleChains.jl.","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"model = chain(\n    Static(1),\n    Dense(10, activation_fn=tanh),\n    Dense(10, activation_fn=sigmoid),\n    Dense(1, activation_fn=identity),\n);\nnothing # hide","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"Here, we specify the expected feature size of the input, leaving out the batch dimension.","category":"page"},{"location":"getting_started/#Inference-(Forward-Pass)","page":"Getting Started","title":"Inference (Forward-Pass)","text":"","category":"section"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"To run inference with this model, we first need to preallocate a buffer to store the intermediate forward pass values. This preallocation is by design, so that memory is only allocated once at the beginning of training.","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"forward_buffer = preallocate(model, batch_size);\nnothing # hide","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"This buffer also contains the input to the neural network. We can set the inputs to the neural network via","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"set_inputs!(forward_buffer, x);\nnothing # hide","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"The above function can be used to set the new inputs at each epoch.","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"We can access the flat parameter vector of the model via parameters(model) to initialise the weights of the network, i.e.","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"using Random\nRandom.seed!(1234)\nparams = parameters(model);\nrandn!(params);\nparams .*= 0.1;\nnothing # hide","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"We can run inference with","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"forward!(forward_buffer, model);\nyhat = get_outputs(forward_buffer);\nnothing # hide","category":"page"},{"location":"getting_started/#Training-(Backward-Pass)","page":"Getting Started","title":"Training (Backward-Pass)","text":"","category":"section"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"We can specify a mean-squared error loss via","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"loss = MSELoss(y);\nnothing # hide","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"and preallocate the buffer used for calculating the gradients via back-propagation:","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"gradient_buffer = preallocate_grads(model, batch_size);\nnothing # hide","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"Now we have all the ingredients we need to write a simple training script, making use of the built-in optimisers.","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"# Setup Adam optimiser\noptimiser = AdamOptimiser(gradient_buffer.parameter_gradients; lr=0.01f0)\n\nepochs = 1000\nlosses = zeros(Float32, epochs)\nfor i in 1:epochs\n    forward!(forward_buffer, model)\n    losses[i] = backprop!(gradient_buffer, forward_buffer, model, loss)\n    grads = gradients(gradient_buffer) # extract the gradient vector\n    # Apply the optimiser\n    update!(params, grads, optimiser)\nend\n\nnothing # hide","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"We can plot the losses over time, for example using Plots.jl:","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"using Plots\nusing PlotThemes # hide\ntheme(:dark) # hide\nplot(losses, xlabel=\"Epochs\", ylabel=\"MSE Loss\", lw=2, label=nothing)","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"Finally, we can run one final forward pass to get the predictions","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"forward!(forward_buffer, model);\nyhat = get_outputs(forward_buffer);\nnothing # hide","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"and then plot the predictions","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"using Plots\nplt = plot(x', y', linestyle=:solid, label=\"Original\", lw=2);\nplot!(plt, x', yhat', linestyle=:dashdot, label=\"Prediction\", lw=2);\nxlabel!(\"x\")\nylabel!(\"y\")\nplt","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"To see an example using convolution layers and GPU training, see the MNIST training example.","category":"page"},{"location":"function_index/#Core-Functions","page":"Index","title":"Core Functions","text":"","category":"section"},{"location":"function_index/#Model-Creation","page":"Index","title":"Model Creation","text":"","category":"section"},{"location":"function_index/","page":"Index","title":"Index","text":"chain - Create a neural network model\nparameters - Access model parameters","category":"page"},{"location":"function_index/#Layer-Types","page":"Index","title":"Layer Types","text":"","category":"section"},{"location":"function_index/","page":"Index","title":"Index","text":"Static - Input layer specifying dimensions\nDense - Fully connected layer\nConv - Convolutional layer\nMaxPool - Max pooling layer\nFlatten - Flatten multi-dimensional data","category":"page"},{"location":"function_index/#Forward-Pass","page":"Index","title":"Forward Pass","text":"","category":"section"},{"location":"function_index/","page":"Index","title":"Index","text":"preallocate - Create forward pass cache\nset_inputs! - Set input data\nforward! - Execute forward pass\nget_outputs - Extract model outputs","category":"page"},{"location":"function_index/#Backward-Pass","page":"Index","title":"Backward Pass","text":"","category":"section"},{"location":"function_index/","page":"Index","title":"Index","text":"preallocate_grads - Create gradient cache\nbackprop! - Execute backward pass\ngradients - Access computed gradients","category":"page"},{"location":"function_index/#Loss-Functions","page":"Index","title":"Loss Functions","text":"","category":"section"},{"location":"function_index/","page":"Index","title":"Index","text":"MSELoss - Mean squared error loss\nLogitCrossEntropyLoss - Cross entropy loss for classification","category":"page"},{"location":"function_index/#Activation-Functions","page":"Index","title":"Activation Functions","text":"","category":"section"},{"location":"function_index/","page":"Index","title":"Index","text":"relu - ReLU activation\nsigmoid - Sigmoid activation\ntanh_fast - Fast tanh activation","category":"page"},{"location":"function_index/#GPU-Support","page":"Index","title":"GPU Support","text":"","category":"section"},{"location":"function_index/","page":"Index","title":"Index","text":"gpu - Move models/data to GPU","category":"page"},{"location":"function_index/#Function-Index","page":"Index","title":"Function Index","text":"","category":"section"},{"location":"function_index/","page":"Index","title":"Index","text":"","category":"page"},{"location":"gpu_usage/#GPU-Usage","page":"GPU Usage","title":"GPU Usage","text":"","category":"section"},{"location":"gpu_usage/","page":"GPU Usage","title":"GPU Usage","text":"SimpleNNs.jl provides full GPU support through CUDA.jl, allowing you to train and run inference on NVIDIA GPUs for significantly improved performance.","category":"page"},{"location":"gpu_usage/#Prerequisites","page":"GPU Usage","title":"Prerequisites","text":"","category":"section"},{"location":"gpu_usage/","page":"GPU Usage","title":"GPU Usage","text":"To use GPU functionality, you need to have the following packages installed:","category":"page"},{"location":"gpu_usage/","page":"GPU Usage","title":"GPU Usage","text":"using Pkg\nPkg.add([\"CUDA\", \"cuDNN\", \"NNlib\"])","category":"page"},{"location":"gpu_usage/","page":"GPU Usage","title":"GPU Usage","text":"These packages must be loaded before using SimpleNNs GPU functionality:","category":"page"},{"location":"gpu_usage/","page":"GPU Usage","title":"GPU Usage","text":"using CUDA\nimport cuDNN, NNlib # Need to load CUDA, cuDNN and NNlib to enable GPU functionality in SimpleNNs\nusing SimpleNNs","category":"page"},{"location":"gpu_usage/","page":"GPU Usage","title":"GPU Usage","text":"note: GPU Requirements\nYou need an NVIDIA GPU with CUDA Compute Capability 3.5 or higher. Check your GPU compatibility with CUDA.functional().","category":"page"},{"location":"gpu_usage/#Basic-GPU-Usage","page":"GPU Usage","title":"Basic GPU Usage","text":"","category":"section"},{"location":"gpu_usage/#Moving-Models-to-GPU","page":"GPU Usage","title":"Moving Models to GPU","text":"","category":"section"},{"location":"gpu_usage/","page":"GPU Usage","title":"GPU Usage","text":"The simplest way to use GPU acceleration is with the gpu function:","category":"page"},{"location":"gpu_usage/","page":"GPU Usage","title":"GPU Usage","text":"# Create a model on CPU\nmodel = chain(\n    Static(784),  # MNIST flattened images\n    Dense(128, activation_fn=relu),\n    Dense(64, activation_fn=relu),\n    Dense(10, activation_fn=identity)\n)\n\n# Move to GPU\ngpu_model = gpu(model)","category":"page"},{"location":"gpu_usage/#GPU-Data-Transfer","page":"GPU Usage","title":"GPU Data Transfer","text":"","category":"section"},{"location":"gpu_usage/","page":"GPU Usage","title":"GPU Usage","text":"You can move arrays to the GPU using the same gpu function:","category":"page"},{"location":"gpu_usage/","page":"GPU Usage","title":"GPU Usage","text":"# CPU data\ncpu_data = randn(Float32, 784, 100)  # 100 samples\n\n# Move to GPU\ngpu_data = gpu(cpu_data)\n\n# You can also use CUDA.cu() directly\ngpu_data = CUDA.cu(cpu_data)","category":"page"},{"location":"gpu_usage/#Complete-GPU-Training-Example","page":"GPU Usage","title":"Complete GPU Training Example","text":"","category":"section"},{"location":"gpu_usage/","page":"GPU Usage","title":"GPU Usage","text":"Here's a complete example showing GPU training:","category":"page"},{"location":"gpu_usage/","page":"GPU Usage","title":"GPU Usage","text":"using CUDA\nimport cuDNN, NNlib\nusing SimpleNNs\nusing Random\n\n# Check GPU availability\nif !CUDA.functional()\n    error(\"CUDA not available!\")\nend\n\n# Create model and move to GPU\nmodel = chain(\n    Static(10),\n    Dense(32, activation_fn=tanh),\n    Dense(16, activation_fn=relu),\n    Dense(1, activation_fn=identity)\n) |> gpu\n\n# Generate sample data on GPU\nbatch_size = 128\ninputs = CUDA.randn(Float32, 10, batch_size)\ntargets = CUDA.randn(Float32, 1, batch_size)\n\n# Preallocate buffers\nforward_cache = preallocate(model, batch_size)\nbackward_cache = preallocate_grads(model, batch_size)\n\n# Set inputs and create loss\nset_inputs!(forward_cache, inputs)\nloss = MSELoss(targets)\n\n# Initialise parameters\nRandom.seed!(42)\nps = parameters(model)\nrandn!(ps)\nps .*= 0.1f0\n\n# Setup optimiser\noptimiser = AdamOptimiser(backward_cache.parameter_gradients; lr=0.01f0)\n\n# Training loop\nfor epoch in 1:1000\n    forward!(forward_cache, model)\n    total_loss = backprop!(backward_cache, forward_cache, model, loss)\n    \n    # Apply optimiser\n    grads = gradients(backward_cache)\n    update!(ps, grads, optimiser)\n    \n    if epoch % 100 == 0\n        println(\"Epoch $epoch, Loss: $total_loss\")\n    end\nend","category":"page"},{"location":"gpu_usage/#GPU-Performance-Benefits","page":"GPU Usage","title":"GPU Performance Benefits","text":"","category":"section"},{"location":"gpu_usage/#Convolutional-Networks","page":"GPU Usage","title":"Convolutional Networks","text":"","category":"section"},{"location":"gpu_usage/","page":"GPU Usage","title":"GPU Usage","text":"GPU acceleration is particularly beneficial for convolutional networks:","category":"page"},{"location":"gpu_usage/","page":"GPU Usage","title":"GPU Usage","text":"using CUDA, cuDNN, NNlib\nusing SimpleNNs\nusing MLDatasets\n\n# Load MNIST data\ndataset = MNIST(:train)\nimages, labels = dataset[:]\n\n# Reshape and move to GPU\nimages = reshape(images, 28, 28, 1, size(images, 3)) |> gpu\nlabels = (labels .+ 1) |> gpu\n\n# Create CNN model on GPU\nmodel = chain(\n    Static((28, 28, 1)),\n    Conv((5,5), 16, activation_fn=relu),\n    MaxPool((2,2)),\n    Conv((3,3), 8, activation_fn=relu),\n    MaxPool((4,4)),\n    Flatten(),\n    Dense(10, activation_fn=identity)\n) |> gpu\n\nbatch_size = 64\nforward_cache = preallocate(model, batch_size)\nbackward_cache = preallocate_grads(model, batch_size)\n\n# Training with GPU acceleration\nfor epoch in 1:100\n    # Select random batch\n    batch_indices = rand(1:size(images, 4), batch_size)\n    batch_images = view(images, :, :, :, batch_indices)\n    batch_labels = view(labels, batch_indices)\n    \n    set_inputs!(forward_cache, batch_images)\n    loss = LogitCrossEntropyLoss(batch_labels, 10)\n    \n    forward!(forward_cache, model)\n    total_loss = backprop!(backward_cache, forward_cache, model, loss)\n    \n    # Apply gradients (simplified)\n    ps = parameters(model)\n    grads = gradients(backward_cache)\n    \n    # Use built-in SGD optimiser for demonstration\n    if !@isdefined(sgd_opt)\n        sgd_opt = SGDOptimiser(grads; lr=0.001f0, momentum=0.9f0)\n    end\n    update!(ps, grads, sgd_opt)\nend","category":"page"},{"location":"gpu_usage/#Performance-Considerations","page":"GPU Usage","title":"Performance Considerations","text":"","category":"section"},{"location":"gpu_usage/#GPU-vs-CPU-Comparison","page":"GPU Usage","title":"GPU vs CPU Comparison","text":"","category":"section"},{"location":"gpu_usage/","page":"GPU Usage","title":"GPU Usage","text":"Here's a simple benchmark comparing GPU and CPU performance:","category":"page"},{"location":"gpu_usage/","page":"GPU Usage","title":"GPU Usage","text":"using BenchmarkTools\n\n# Create identical models\ncpu_model = chain(Static(100), Dense(200, activation_fn=relu), Dense(1))\ngpu_model = gpu(cpu_model)\n\nbatch_size = 128\ncpu_cache = preallocate(cpu_model, batch_size)\ngpu_cache = preallocate(gpu_model, batch_size)\n\n# CPU data\ncpu_inputs = randn(Float32, 100, batch_size)\nset_inputs!(cpu_cache, cpu_inputs)\n\n# GPU data\ngpu_inputs = gpu(cpu_inputs)\nset_inputs!(gpu_cache, gpu_inputs)\n\n# Benchmark\nprintln(\"CPU Performance:\")\n@benchmark forward!($cpu_cache, $cpu_model)\n\nprintln(\"GPU Performance:\")\n@benchmark CUDA.@sync forward!($gpu_cache, $gpu_model)","category":"page"},{"location":"gpu_usage/","page":"GPU Usage","title":"GPU Usage","text":"On my machine, I see the following results","category":"page"},{"location":"gpu_usage/","page":"GPU Usage","title":"GPU Usage","text":"CPU Benchmark:\nBenchmarkTools.Trial: 10000 samples with 1 evaluation per sample.\n Range (min … max):   96.800 μs … 386.100 μs  ┊ GC (min … max): 0.00% … 0.00%\n Time  (median):     105.900 μs               ┊ GC (median):    0.00%\n Time  (mean ± σ):   111.648 μs ±  18.544 μs  ┊ GC (mean ± σ):  0.00% ± 0.00%\n\n  ▁▄▆██▇▇▆▆▆▅▄▄▃▃▃▂▂▁▂▂▁▂▁▁▁▁       ▁                           ▂\n  ████████████████████████████████▇████▇█▇██▇▅▇▆▆▅▆▅▃▅▅▄▅▃▅▅▅▄▃ █\n  96.8 μs       Histogram: log(frequency) by time        190 μs <\n\n Memory estimate: 0 bytes, allocs estimate: 0.\n\nGPU Benchmark:\nBenchmarkTools.Trial: 10000 samples with 1 evaluation per sample.\n Range (min … max):  67.000 μs … 619.800 μs  ┊ GC (min … max): 0.00% … 0.00%\n Time  (median):     85.700 μs               ┊ GC (median):    0.00%\n Time  (mean ± σ):   96.932 μs ±  42.793 μs  ┊ GC (mean ± σ):  0.00% ± 0.00%\n\n  ▁▁  █▄\n  ███████▇▇▆▅▅▅▅▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▂▂▂▁▂\n  67 μs           Histogram: frequency by time          275 μs <\n\n Memory estimate: 9.58 KiB, allocs estimate: 342.","category":"page"},{"location":"gpu_usage/","page":"GPU Usage","title":"GPU Usage","text":"You can see these small sizes actually show the GPU and CPU having similar performance. Keep this in mind when you are choosing which device to run your small-medium size neural networks on.","category":"page"},{"location":"#SimpleNNs.jl","page":"Home","title":"SimpleNNs.jl","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"SimpleNNs.jl is heavily inspired by SimpleChains.jl, which showed that there is space for micro-optimisations to be very important for small neural networks (see the blog post). This project aims to expand upon SimpleChains.jl by introducing both CPU and GPU support with zero-allocation inference and training.","category":"page"},{"location":"#Key-Features","page":"Home","title":"Key Features","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Zero-allocation inference: Pre-allocated buffers eliminate memory allocations during forward and backward passes\nGPU acceleration: Full CUDA support for both training and inference\nHigh performance: Optimised implementations for small to medium-sized networks\nMemory efficient: Flat parameter vectors and pre-allocated caches minimise memory usage","category":"page"},{"location":"#Package-Goals","page":"Home","title":"Package Goals","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"As the name suggests, this is not a fully featured neural network library, and most notably, it does not include auto-differentiation capabilities. The specific goals of this package are:","category":"page"},{"location":"","page":"Home","title":"Home","text":"Simple architectures: Build neural networks with dense and convolutional layers\nFlat parameters: All model parameters stored in a single vector for easy manipulation\nPre-allocated computation: Zero-allocation forward and backward passes using pre-allocated buffers\nCross-platform: Execution on both CPU and GPU (CUDA)\nHigh performance: Optimised for small to medium neural networks where micro-optimisations matter","category":"page"},{"location":"#Supported-Features","page":"Home","title":"Supported Features","text":"","category":"section"},{"location":"#Layer-Types","page":"Home","title":"Layer Types","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Dense layers: Fully connected layers with customisable activation functions\nConvolutional layers: 2D convolutions with ReLU, tanh, and sigmoid activations\nPooling layers: Max pooling with configurable pool sizes and strides\nUtility layers: Static input specification and flattening layers","category":"page"},{"location":"#Activation-Functions","page":"Home","title":"Activation Functions","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"ReLU (relu)\nHyperbolic tangent (tanh, tanh_fast)\nLogistic sigmoid (sigmoid)\nIdentity (identity)","category":"page"},{"location":"#Loss-Functions","page":"Home","title":"Loss Functions","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Mean Squared Error (MSELoss)\nCross Entropy Loss (LogitCrossEntropyLoss)","category":"page"},{"location":"#GPU-Support","page":"Home","title":"GPU Support","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Full CUDA acceleration through CUDA.jl, cuDNN.jl, and NNlib.jl\nSeamless CPU/GPU model transfer with the gpu() function\nOptimised GPU kernels for convolution and dense operations","category":"page"},{"location":"#Quick-Start","page":"Home","title":"Quick Start","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Here's a minimal example to get you started:","category":"page"},{"location":"","page":"Home","title":"Home","text":"using SimpleNNs\n\n# Create a simple neural network\nmodel = chain(\n    Static(4),                          # 4 input features\n    Dense(8, activation_fn=relu),       # Hidden layer with ReLU\n    Dense(1, activation_fn=identity)    # Output layer\n)\n\n# Generate some data\nbatch_size = 32\ninputs = randn(Float32, 4, batch_size)\ntargets = randn(Float32, 1, batch_size)\n\n# Pre-allocate computation buffers\nforward_cache = preallocate(model, batch_size)\nbackward_cache = preallocate_grads(model, batch_size)\n\n# Set inputs and run forward pass\nset_inputs!(forward_cache, inputs)\nforward!(forward_cache, model)\noutputs = get_outputs(forward_cache)\n\n# Define loss and run backward pass\nloss = MSELoss(targets)\ntotal_loss = backprop!(backward_cache, forward_cache, model, loss)\n\n# Access gradients\ngrads = gradients(backward_cache)","category":"page"},{"location":"#Performance-Philosophy","page":"Home","title":"Performance Philosophy","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"SimpleNNs.jl is designed around the principle that for small to medium neural networks, careful memory management and micro-optimisations can provide significant performance benefits. Key design decisions include:","category":"page"},{"location":"","page":"Home","title":"Home","text":"Pre-allocation: All memory is allocated upfront, eliminating allocations during computation\nFlat parameters: Single parameter vector enables efficient gradient updates and serialisation  \nType stability: Careful type design ensures fast, predictable performance\nGPU optimisation: Custom CUDA kernels and NNlib integration for accelerated computation","category":"page"},{"location":"#When-to-Use-SimpleNNs.jl","page":"Home","title":"When to Use SimpleNNs.jl","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"SimpleNNs.jl is ideal for:","category":"page"},{"location":"","page":"Home","title":"Home","text":"Small to medium networks where performance matters\nEmbedded applications requiring minimal memory footprint\nResearch applications needing fine control over memory and computation\nGPU-accelerated inference with minimal overhead\nApplications requiring many small models (e.g., ensemble methods)","category":"page"},{"location":"","page":"Home","title":"Home","text":"Consider other frameworks like Flux.jl or Lux.jl for:","category":"page"},{"location":"","page":"Home","title":"Home","text":"Very large deep learning models\nComplex architectures requiring auto-differentiation\nResearch requiring cutting-edge layer types\nApplications where development speed > runtime performance","category":"page"},{"location":"#Documentation-Structure","page":"Home","title":"Documentation Structure","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Pages = [\n    \"getting_started.md\",\n    \"mnist.md\", \n    \"gpu_usage.md\",\n    \"advanced_usage.md\",\n    \"api.md\",\n    \"function_index.md\"\n]\nDepth = 2","category":"page"},{"location":"#Installation","page":"Home","title":"Installation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Add the package using Julia's package manager:","category":"page"},{"location":"","page":"Home","title":"Home","text":"using Pkg\nPkg.add(\"https://github.com/JamieMair/SimpleNNs.jl\")","category":"page"},{"location":"","page":"Home","title":"Home","text":"For GPU support, also install the CUDA ecosystem:","category":"page"},{"location":"","page":"Home","title":"Home","text":"Pkg.add([\"CUDA\", \"cuDNN\", \"NNlib\"])","category":"page"},{"location":"#Contributing","page":"Home","title":"Contributing","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Contributions are welcome! Please see the GitHub repository for issue tracking and pull requests.","category":"page"},{"location":"#Acknowledgments","page":"Home","title":"Acknowledgments","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"This package is inspired by and builds upon the excellent work in:","category":"page"},{"location":"","page":"Home","title":"Home","text":"SimpleChains.jl - The original high-performance \"small\" neural network package\nNNLib.jl - Provides the CUDA implementation for some of the supported layers\nCUDA.jl - Allows seamless GPU support in this package","category":"page"},{"location":"mnist/#MNIST-Classification","page":"MNIST","title":"MNIST Classification","text":"","category":"section"},{"location":"mnist/","page":"MNIST","title":"MNIST","text":"In this basic example, we'll train a convolutional neural network to classify handwritten digits from the MNIST dataset. This example demonstrates both CPU and GPU training, data preprocessing, and model evaluation.","category":"page"},{"location":"mnist/","page":"MNIST","title":"MNIST","text":"This tutorial will cover:","category":"page"},{"location":"mnist/","page":"MNIST","title":"MNIST","text":"Loading and preprocessing MNIST data\nCreating a CNN architecture with SimpleNNs.jl\nSetting up GPU acceleration (optional)\nImplementing a complete training loop\nEvaluating model performance\nVisualising results and analysis","category":"page"},{"location":"mnist/#Dataset-Preparation","page":"MNIST","title":"Dataset Preparation","text":"","category":"section"},{"location":"mnist/","page":"MNIST","title":"MNIST","text":"First, let's load and prepare the MNIST dataset using MLDatasets.jl:","category":"page"},{"location":"mnist/","page":"MNIST","title":"MNIST","text":"using MLDatasets\nusing SimpleNNs\nusing Random\n\n# Load the training dataset\ndataset = MNIST(:train)\nimages, labels = dataset[:]\n\n# Reshape images to add channel dimension: (height, width, channels, batch)\nimages = reshape(images, size(images, 1), size(images, 2), 1, size(images, 3))\n\n# Convert labels to 1-indexed (MNIST uses 0-9, we need 1-10)\nlabels .+= 1\n\nprintln(\"Dataset shape: \", size(images))\nprintln(\"Labels range: \", extrema(labels))","category":"page"},{"location":"mnist/#GPU-Setup-(Optional)","page":"MNIST","title":"GPU Setup (Optional)","text":"","category":"section"},{"location":"mnist/","page":"MNIST","title":"MNIST","text":"If you have an NVIDIA GPU and want to use GPU acceleration, set up the necessary packages:","category":"page"},{"location":"mnist/","page":"MNIST","title":"MNIST","text":"# Only run this if you have CUDA capable hardware\nusing CUDA, cuDNN, NNlib\n\n# Check if CUDA is functional\ngpu_available = CUDA.functional()\nprintln(\"GPU available: \", gpu_available)\n\n# Define device transfer function\nuse_gpu = gpu_available  # Set to false to force CPU usage\nto_device = use_gpu ? gpu : identity\n\n# Move data to chosen device\nimages = images |> to_device\nlabels = labels |> to_device\n\nprintln(\"Training on: \", use_gpu ? \"GPU\" : \"CPU\")","category":"page"},{"location":"mnist/#Model-Architecture","page":"MNIST","title":"Model Architecture","text":"","category":"section"},{"location":"mnist/","page":"MNIST","title":"MNIST","text":"We'll create a convolutional neural network with the following architecture:","category":"page"},{"location":"mnist/","page":"MNIST","title":"MNIST","text":"Convolutional layer: 5×5 kernels, 16 output channels, ReLU activation\nMax pooling: 2×2 pool size\nConvolutional layer: 3×3 kernels, 8 output channels, ReLU activation  \nMax pooling: 4×4 pool size\nFlatten layer\nDense layer: 32 units, ReLU activation\nOutput layer: 10 units (one per digit class)","category":"page"},{"location":"mnist/","page":"MNIST","title":"MNIST","text":"# Get image dimensions (excluding batch dimension)\nimg_size = size(images)[1:end-1]  # (28, 28, 1)\n\nmodel = chain(\n    Static(img_size),\n    Conv((5,5), 16; activation_fn=relu),\n    MaxPool((2,2)),\n    Conv((3,3), 8; activation_fn=relu),\n    MaxPool((4,4)),\n    Flatten(),\n    Dense(32, activation_fn=relu),\n    Dense(10, activation_fn=identity)  # 10 classes for digits 0-9\n) |> to_device\n\nprintln(\"Model created with \", length(parameters(model)), \" parameters\")","category":"page"},{"location":"mnist/#Training-Setup","page":"MNIST","title":"Training Setup","text":"","category":"section"},{"location":"mnist/","page":"MNIST","title":"MNIST","text":"Set up the training infrastructure with preallocated buffers and optimiser:","category":"page"},{"location":"mnist/","page":"MNIST","title":"MNIST","text":"# Training hyperparameters\nbatch_size = 64\nlearning_rate = 0.01\nepochs = 1000\n\n# Preallocate forward and backward buffers\nforward_buffer = preallocate(model, batch_size)\ngradient_buffer = preallocate_grads(model, batch_size)\n\n# Initialise model parameters\nRandom.seed!(1234)\nparams = parameters(model)\nrandn!(params)\nparams .*= 0.05f0  # Small initial weights\n\n# Setup Adam optimiser using built-in SimpleNNs optimiser\noptimiser = AdamOptimiser(gradient_buffer.parameter_gradients; lr=learning_rate)\n\nprintln(\"Training setup complete\")","category":"page"},{"location":"mnist/#Training-Loop","page":"MNIST","title":"Training Loop","text":"","category":"section"},{"location":"mnist/","page":"MNIST","title":"MNIST","text":"Now we'll implement the training loop with mini-batch stochastic gradient descent:","category":"page"},{"location":"mnist/","page":"MNIST","title":"MNIST","text":"using ProgressBars\n\n# Track training progress\nlosses = Float32[]\ntraining_indices = collect(1:size(images, 4))\n\nprintln(\"Starting training for $epochs epochs...\")\n\nfor epoch in ProgressBar(1:epochs)\n    # Randomly shuffle and select a batch\n    Random.shuffle!(training_indices)\n    batch_indices = view(training_indices, 1:batch_size)\n    \n    # Extract current batch\n    batch_images = view(images, :, :, :, batch_indices)\n    batch_labels = view(labels, batch_indices)\n    \n    # Set inputs for forward pass\n    set_inputs!(forward_buffer, batch_images)\n    \n    # Create loss function for this batch\n    loss_fn = LogitCrossEntropyLoss(batch_labels, 10)\n    \n    # Forward pass\n    forward!(forward_buffer, model)\n    \n    # Backward pass and loss computation\n    current_loss = backprop!(gradient_buffer, forward_buffer, model, loss_fn)\n    push!(losses, current_loss)\n    \n    # Extract gradients and apply optimiser\n    grads = gradients(gradient_buffer)\n    update!(params, grads, optimiser)\n    \n    # Print progress every 100 epochs\n    if epoch % 100 == 0\n        println(\"Epoch $epoch: Loss = $(round(current_loss, digits=4))\")\n    end\nend\n\nprintln(\"Training completed!\")","category":"page"},{"location":"mnist/#Visualising-Training-Progress","page":"MNIST","title":"Visualising Training Progress","text":"","category":"section"},{"location":"mnist/","page":"MNIST","title":"MNIST","text":"You are free to use any package to visualise the training curves.","category":"page"},{"location":"mnist/","page":"MNIST","title":"MNIST","text":"One popular option in the Julia ecosystem is Plots.jl, with the below example:","category":"page"},{"location":"mnist/","page":"MNIST","title":"MNIST","text":"using Plots\n\n# Plot training loss\nplot(losses, \n     xlabel=\"Epoch\", \n     ylabel=\"Cross Entropy Loss\", \n     title=\"MNIST Training Progress\",\n     lw=2, \n     label=\"Training Loss\",\n     yscale=:log10)  # Log scale for better visualisation","category":"page"},{"location":"mnist/","page":"MNIST","title":"MNIST","text":"One can also use TensorBoardLogger.jl to plot and view training updates in real time - but this requires installing tensorboard separately in Python.","category":"page"},{"location":"mnist/#Model-Evaluation","page":"MNIST","title":"Model Evaluation","text":"","category":"section"},{"location":"mnist/","page":"MNIST","title":"MNIST","text":"Evaluate the trained model on the test set:","category":"page"},{"location":"mnist/","page":"MNIST","title":"MNIST","text":"# Load test dataset\ntest_dataset = MNIST(:test)\ntest_images, test_labels = test_dataset[:]\n\n# Preprocess test data same way as training data\ntest_images = reshape(test_images, size(test_images, 1), size(test_images, 2), 1, size(test_images, 3))\ntest_labels .+= 1  # Convert to 1-indexed\ntest_images = test_images |> to_device\ntest_labels = test_labels |> to_device\n\nprintln(\"Test set size: \", size(test_images))","category":"page"},{"location":"mnist/#Inference-on-Test-Set","page":"MNIST","title":"Inference on Test Set","text":"","category":"section"},{"location":"mnist/","page":"MNIST","title":"MNIST","text":"# Create forward buffer for entire test set\nn_test = size(test_images, 4)\ntest_forward_buffer = preallocate(model, n_test)\n\n# Run inference\nset_inputs!(test_forward_buffer, test_images)\nforward!(test_forward_buffer, model)\n\n# Get predictions\nlogits = get_outputs(test_forward_buffer)\npredictions = [argmax(col)[1] for col in eachcol(Array(logits))]\n\n# Convert back to CPU for analysis if needed\ncpu_test_labels = Array(test_labels)\ncpu_predictions = predictions\n\n# Calculate accuracy\ncorrect_predictions = sum(cpu_predictions .== cpu_test_labels)\naccuracy = correct_predictions / length(cpu_test_labels) * 100\n\nprintln(\"Test Accuracy: $(round(accuracy, digits=2))%\")\nprintln(\"Correct: $correct_predictions / $(length(cpu_test_labels))\")","category":"page"},{"location":"mnist/#Detailed-Analysis","page":"MNIST","title":"Detailed Analysis","text":"","category":"section"},{"location":"mnist/","page":"MNIST","title":"MNIST","text":"Create a confusion matrix to analyze model performance:","category":"page"},{"location":"mnist/","page":"MNIST","title":"MNIST","text":"# Create confusion matrix\nfunction confusion_matrix(y_true, y_pred, n_classes=10)\n    cm = zeros(Int, n_classes, n_classes)\n    for (true_label, pred_label) in zip(y_true, y_pred)\n        cm[true_label, pred_label] += 1\n    end\n    return cm\nend\n\ncm = confusion_matrix(cpu_test_labels, cpu_predictions)\n\n# Display confusion matrix\nprintln(\"Confusion Matrix:\")\nprintln(\"Rows: True labels, Columns: Predicted labels\")\nfor i in 1:10\n    println(\"Class $(i-1): \", join(cm[i, :], \"\\t\"))\nend\n\n# Per-class accuracy\nclass_accuracies = [cm[i,i] / sum(cm[i,:]) for i in 1:10]\nfor (i, acc) in enumerate(class_accuracies)\n    println(\"Digit $(i-1) accuracy: $(round(acc*100, digits=1))%\")\nend","category":"page"},{"location":"mnist/#Visualising-Predictions","page":"MNIST","title":"Visualising Predictions","text":"","category":"section"},{"location":"mnist/","page":"MNIST","title":"MNIST","text":"Let's visualise some test examples with predictions:","category":"page"},{"location":"mnist/","page":"MNIST","title":"MNIST","text":"using Plots\n\n# Function to display digit images\nfunction plot_digit_predictions(images, true_labels, predictions, indices)\n    n = length(indices)\n    plots = []\n    \n    for i in 1:min(n, 16)  # Show up to 16 examples\n        idx = indices[i]\n        img = Array(images[:, :, 1, idx])\n        true_label = true_labels[idx] - 1  # Convert back to 0-9\n        pred_label = predictions[idx] - 1\n        \n        is_correct = true_label == pred_label\n        title_color = is_correct ? :green : :red\n        \n        p = heatmap(img', \n                   color=:grays, \n                   aspect_ratio=:equal,\n                   title=\"T:$true_label P:$pred_label\",\n                   titlefontcolor=title_color,\n                   showaxis=false,\n                   grid=false)\n        push!(plots, p)\n    end\n    \n    plot(plots..., layout=(4, 4), size=(800, 800))\nend\n\n# Show some random predictions\nrandom_indices = rand(1:n_test, 16)\nplot_digit_predictions(test_images, cpu_test_labels, cpu_predictions, random_indices)","category":"page"}]
}
