<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Home · SimpleNNs.jl</title><meta name="title" content="Home · SimpleNNs.jl"/><meta property="og:title" content="Home · SimpleNNs.jl"/><meta property="twitter:title" content="Home · SimpleNNs.jl"/><meta name="description" content="Documentation for SimpleNNs.jl."/><meta property="og:description" content="Documentation for SimpleNNs.jl."/><meta property="twitter:description" content="Documentation for SimpleNNs.jl."/><meta property="og:url" content="https://JamieMair.github.io/SimpleNNs.jl/"/><meta property="twitter:url" content="https://JamieMair.github.io/SimpleNNs.jl/"/><link rel="canonical" href="https://JamieMair.github.io/SimpleNNs.jl/"/><script data-outdated-warner src="assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="assets/documenter.js"></script><script src="search_index.js"></script><script src="siteinfo.js"></script><script src="../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href>SimpleNNs.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li class="is-active"><a class="tocitem" href>Home</a><ul class="internal"><li><a class="tocitem" href="#Key-Features"><span>Key Features</span></a></li><li><a class="tocitem" href="#Package-Goals"><span>Package Goals</span></a></li><li><a class="tocitem" href="#Supported-Features"><span>Supported Features</span></a></li><li><a class="tocitem" href="#Quick-Start"><span>Quick Start</span></a></li><li><a class="tocitem" href="#Performance-Philosophy"><span>Performance Philosophy</span></a></li><li><a class="tocitem" href="#When-to-Use-SimpleNNs.jl"><span>When to Use SimpleNNs.jl</span></a></li><li><a class="tocitem" href="#Documentation-Structure"><span>Documentation Structure</span></a></li><li><a class="tocitem" href="#Installation"><span>Installation</span></a></li><li><a class="tocitem" href="#Contributing"><span>Contributing</span></a></li><li><a class="tocitem" href="#Acknowledgments"><span>Acknowledgments</span></a></li></ul></li><li><a class="tocitem" href="getting_started/">Getting Started</a></li><li><span class="tocitem">Examples</span><ul><li><a class="tocitem" href="mnist/">MNIST</a></li></ul></li><li><span class="tocitem">Advanced Topics</span><ul><li><a class="tocitem" href="initialisation/">Parameter Initialisation</a></li><li><a class="tocitem" href="gpu_usage/">GPU Usage</a></li><li><a class="tocitem" href="advanced_usage/">Advanced Usage</a></li></ul></li><li><a class="tocitem" href="api/">API</a></li><li><a class="tocitem" href="function_index/">Index</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Home</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Home</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/JamieMair/SimpleNNs.jl/blob/main/docs/src/index.md#" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="SimpleNNs.jl"><a class="docs-heading-anchor" href="#SimpleNNs.jl">SimpleNNs.jl</a><a id="SimpleNNs.jl-1"></a><a class="docs-heading-anchor-permalink" href="#SimpleNNs.jl" title="Permalink"></a></h1><p><code>SimpleNNs.jl</code> is heavily inspired by <a href="https://pumasai.github.io/SimpleChains.jl/stable/"><code>SimpleChains.jl</code></a>, which showed that there is space for micro-optimisations to be very important for small neural networks (see the <a href="https://julialang.org/blog/2022/04/simple-chains/">blog post</a>). This project aims to expand upon <code>SimpleChains.jl</code> by introducing both CPU and GPU support with zero-allocation inference and training.</p><h2 id="Key-Features"><a class="docs-heading-anchor" href="#Key-Features">Key Features</a><a id="Key-Features-1"></a><a class="docs-heading-anchor-permalink" href="#Key-Features" title="Permalink"></a></h2><ul><li><strong>Zero-allocation inference</strong>: Pre-allocated buffers eliminate memory allocations during forward and backward passes</li><li><strong>GPU acceleration</strong>: Full CUDA support for both training and inference</li><li><strong>High performance</strong>: Optimised implementations for small to medium-sized networks</li><li><strong>Memory efficient</strong>: Flat parameter vectors and pre-allocated caches minimise memory usage</li></ul><h2 id="Package-Goals"><a class="docs-heading-anchor" href="#Package-Goals">Package Goals</a><a id="Package-Goals-1"></a><a class="docs-heading-anchor-permalink" href="#Package-Goals" title="Permalink"></a></h2><p>As the name suggests, this is <strong>not</strong> a fully featured neural network library, and most notably, it does not include auto-differentiation capabilities. The specific goals of this package are:</p><ol><li><strong>Simple architectures</strong>: Build neural networks with dense and convolutional layers</li><li><strong>Flat parameters</strong>: All model parameters stored in a single vector for easy manipulation</li><li><strong>Pre-allocated computation</strong>: Zero-allocation forward and backward passes using pre-allocated buffers</li><li><strong>Cross-platform</strong>: Execution on both CPU and GPU (CUDA)</li><li><strong>High performance</strong>: Optimised for small to medium neural networks where micro-optimisations matter</li></ol><h2 id="Supported-Features"><a class="docs-heading-anchor" href="#Supported-Features">Supported Features</a><a id="Supported-Features-1"></a><a class="docs-heading-anchor-permalink" href="#Supported-Features" title="Permalink"></a></h2><h3 id="Layer-Types"><a class="docs-heading-anchor" href="#Layer-Types">Layer Types</a><a id="Layer-Types-1"></a><a class="docs-heading-anchor-permalink" href="#Layer-Types" title="Permalink"></a></h3><ul><li><strong>Dense layers</strong>: Fully connected layers with customisable activation functions</li><li><strong>Convolutional layers</strong>: 2D convolutions with ReLU, tanh, and sigmoid activations</li><li><strong>Pooling layers</strong>: Max pooling with configurable pool sizes and strides</li><li><strong>Utility layers</strong>: Static input specification and flattening layers</li></ul><h3 id="Activation-Functions"><a class="docs-heading-anchor" href="#Activation-Functions">Activation Functions</a><a id="Activation-Functions-1"></a><a class="docs-heading-anchor-permalink" href="#Activation-Functions" title="Permalink"></a></h3><ul><li>ReLU (<code>relu</code>)</li><li>Hyperbolic tangent (<code>tanh</code>, <code>tanh_fast</code>)</li><li>Logistic sigmoid (<code>sigmoid</code>)</li><li>Identity (<code>identity</code>)</li></ul><h3 id="Loss-Functions"><a class="docs-heading-anchor" href="#Loss-Functions">Loss Functions</a><a id="Loss-Functions-1"></a><a class="docs-heading-anchor-permalink" href="#Loss-Functions" title="Permalink"></a></h3><ul><li>Mean Squared Error (<code>MSELoss</code>)</li><li>Cross Entropy Loss (<code>LogitCrossEntropyLoss</code>)</li></ul><h3 id="GPU-Support"><a class="docs-heading-anchor" href="#GPU-Support">GPU Support</a><a id="GPU-Support-1"></a><a class="docs-heading-anchor-permalink" href="#GPU-Support" title="Permalink"></a></h3><ul><li>Full CUDA acceleration through <code>CUDA.jl</code>, <code>cuDNN.jl</code>, and <code>NNlib.jl</code></li><li>Seamless CPU/GPU model transfer with the <code>gpu()</code> function</li><li>Optimised GPU kernels for convolution and dense operations</li></ul><h2 id="Quick-Start"><a class="docs-heading-anchor" href="#Quick-Start">Quick Start</a><a id="Quick-Start-1"></a><a class="docs-heading-anchor-permalink" href="#Quick-Start" title="Permalink"></a></h2><p>Here&#39;s a minimal example to get you started:</p><pre><code class="language-julia hljs">using SimpleNNs

# Create a simple neural network
model = chain(
    Static(4),                          # 4 input features
    Dense(8, activation_fn=relu),       # Hidden layer with ReLU
    Dense(1, activation_fn=identity)    # Output layer
)

# Generate some data
batch_size = 32
inputs = randn(Float32, 4, batch_size)
targets = randn(Float32, 1, batch_size)

# Pre-allocate computation buffers
forward_cache = preallocate(model, batch_size)
backward_cache = preallocate_grads(model, batch_size)

# Set inputs and run forward pass
set_inputs!(forward_cache, inputs)
forward!(forward_cache, model)
outputs = get_outputs(forward_cache)

# Define loss and run backward pass
loss = MSELoss(targets)
total_loss = backprop!(backward_cache, forward_cache, model, loss)

# Access gradients
grads = gradients(backward_cache)</code></pre><h2 id="Performance-Philosophy"><a class="docs-heading-anchor" href="#Performance-Philosophy">Performance Philosophy</a><a id="Performance-Philosophy-1"></a><a class="docs-heading-anchor-permalink" href="#Performance-Philosophy" title="Permalink"></a></h2><p>SimpleNNs.jl is designed around the principle that for small to medium neural networks, careful memory management and micro-optimisations can provide significant performance benefits. Key design decisions include:</p><ul><li><strong>Pre-allocation</strong>: All memory is allocated upfront, eliminating allocations during computation</li><li><strong>Flat parameters</strong>: Single parameter vector enables efficient gradient updates and serialisation  </li><li><strong>Type stability</strong>: Careful type design ensures fast, predictable performance</li><li><strong>GPU optimisation</strong>: Custom CUDA kernels and NNlib integration for accelerated computation</li></ul><h2 id="When-to-Use-SimpleNNs.jl"><a class="docs-heading-anchor" href="#When-to-Use-SimpleNNs.jl">When to Use SimpleNNs.jl</a><a id="When-to-Use-SimpleNNs.jl-1"></a><a class="docs-heading-anchor-permalink" href="#When-to-Use-SimpleNNs.jl" title="Permalink"></a></h2><p>SimpleNNs.jl is ideal for:</p><ul><li><strong>Small to medium networks</strong> where performance matters</li><li><strong>Embedded applications</strong> requiring minimal memory footprint</li><li><strong>Research applications</strong> needing fine control over memory and computation</li><li><strong>GPU-accelerated inference</strong> with minimal overhead</li><li><strong>Applications requiring many small models</strong> (e.g., ensemble methods)</li></ul><p>Consider other frameworks like Flux.jl or Lux.jl for:</p><ul><li>Very large deep learning models</li><li>Complex architectures requiring auto-differentiation</li><li>Research requiring cutting-edge layer types</li><li>Applications where development speed &gt; runtime performance</li></ul><h2 id="Documentation-Structure"><a class="docs-heading-anchor" href="#Documentation-Structure">Documentation Structure</a><a id="Documentation-Structure-1"></a><a class="docs-heading-anchor-permalink" href="#Documentation-Structure" title="Permalink"></a></h2><ul><li><a href="getting_started/#Getting-Started">Getting Started</a></li><li class="no-marker"><ul><li><a href="getting_started/#Inference-(Forward-Pass)">Inference (Forward-Pass)</a></li><li><a href="getting_started/#Training-(Backward-Pass)">Training (Backward-Pass)</a></li></ul></li><li><a href="mnist/#MNIST-Classification">MNIST Classification</a></li><li class="no-marker"><ul><li><a href="mnist/#Dataset-Preparation">Dataset Preparation</a></li><li><a href="mnist/#GPU-Setup-(Optional)">GPU Setup (Optional)</a></li><li><a href="mnist/#Model-Architecture">Model Architecture</a></li><li><a href="mnist/#Training-Setup">Training Setup</a></li><li><a href="mnist/#Training-Loop">Training Loop</a></li><li><a href="mnist/#Visualising-Training-Progress">Visualising Training Progress</a></li><li><a href="mnist/#Model-Evaluation">Model Evaluation</a></li></ul></li><li><a href="gpu_usage/#GPU-Usage">GPU Usage</a></li><li class="no-marker"><ul><li><a href="gpu_usage/#Prerequisites">Prerequisites</a></li><li><a href="gpu_usage/#Basic-GPU-Usage">Basic GPU Usage</a></li><li><a href="gpu_usage/#GPU-Performance-Benefits">GPU Performance Benefits</a></li><li><a href="gpu_usage/#Performance-Considerations">Performance Considerations</a></li><li><a href="gpu_usage/#GPU-vs-CPU-Comparison">GPU vs CPU Comparison</a></li></ul></li><li><a href="advanced_usage/#Advanced-Usage">Advanced Usage</a></li><li class="no-marker"><ul><li><a href="advanced_usage/#Custom-Training-Loops">Custom Training Loops</a></li><li><a href="advanced_usage/#Memory-Optimisation">Memory Optimisation</a></li><li><a href="advanced_usage/#Performance-Profiling">Performance Profiling</a></li><li><a href="advanced_usage/#Custom-Activation-Functions">Custom Activation Functions</a></li><li><a href="advanced_usage/#Model-Serialisation">Model Serialisation</a></li></ul></li><li><a href="api/#API">API</a></li><li><a href="function_index/#Core-Functions">Core Functions</a></li><li class="no-marker"><ul><li><a href="function_index/#Model-Creation">Model Creation</a></li><li><a href="function_index/#Layer-Types">Layer Types</a></li><li><a href="function_index/#Forward-Pass">Forward Pass</a></li><li><a href="function_index/#Backward-Pass">Backward Pass</a></li><li><a href="function_index/#Loss-Functions">Loss Functions</a></li><li><a href="function_index/#Activation-Functions">Activation Functions</a></li><li><a href="function_index/#GPU-Support">GPU Support</a></li></ul></li><li><a href="function_index/#Function-Index">Function Index</a></li></ul><h2 id="Installation"><a class="docs-heading-anchor" href="#Installation">Installation</a><a id="Installation-1"></a><a class="docs-heading-anchor-permalink" href="#Installation" title="Permalink"></a></h2><p>Add the package using Julia&#39;s package manager:</p><pre><code class="language-julia hljs">using Pkg
Pkg.add(&quot;https://github.com/JamieMair/SimpleNNs.jl&quot;)</code></pre><p>For GPU support, also install the CUDA ecosystem:</p><pre><code class="language-julia hljs">Pkg.add([&quot;CUDA&quot;, &quot;cuDNN&quot;, &quot;NNlib&quot;])</code></pre><h2 id="Contributing"><a class="docs-heading-anchor" href="#Contributing">Contributing</a><a id="Contributing-1"></a><a class="docs-heading-anchor-permalink" href="#Contributing" title="Permalink"></a></h2><p>Contributions are welcome! Please see the GitHub repository for issue tracking and pull requests.</p><h2 id="Acknowledgments"><a class="docs-heading-anchor" href="#Acknowledgments">Acknowledgments</a><a id="Acknowledgments-1"></a><a class="docs-heading-anchor-permalink" href="#Acknowledgments" title="Permalink"></a></h2><p>This package is inspired by and builds upon the excellent work in:</p><ul><li><a href="https://github.com/PumasAI/SimpleChains.jl"><code>SimpleChains.jl</code></a> - The original high-performance &quot;small&quot; neural network package</li><li><a href="https://fluxml.ai/NNlib.jl/stable/"><code>NNLib.jl</code></a> - Provides the CUDA implementation for some of the supported layers</li><li><a href="https://github.com/JuliaGPU/CUDA.jl"><code>CUDA.jl</code></a> - Allows seamless GPU support in this package</li></ul></article><nav class="docs-footer"><a class="docs-footer-nextpage" href="getting_started/">Getting Started »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.13.0 on <span class="colophon-date" title="Tuesday 17 February 2026 11:34">Tuesday 17 February 2026</span>. Using Julia version 1.11.9.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
