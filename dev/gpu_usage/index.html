<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>GPU Usage · SimpleNNs.jl</title><meta name="title" content="GPU Usage · SimpleNNs.jl"/><meta property="og:title" content="GPU Usage · SimpleNNs.jl"/><meta property="twitter:title" content="GPU Usage · SimpleNNs.jl"/><meta name="description" content="Documentation for SimpleNNs.jl."/><meta property="og:description" content="Documentation for SimpleNNs.jl."/><meta property="twitter:description" content="Documentation for SimpleNNs.jl."/><meta property="og:url" content="https://JamieMair.github.io/SimpleNNs.jl/gpu_usage/"/><meta property="twitter:url" content="https://JamieMair.github.io/SimpleNNs.jl/gpu_usage/"/><link rel="canonical" href="https://JamieMair.github.io/SimpleNNs.jl/gpu_usage/"/><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../search_index.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../">SimpleNNs.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li><a class="tocitem" href="../getting_started/">Getting Started</a></li><li><span class="tocitem">Examples</span><ul><li><a class="tocitem" href="../mnist/">MNIST</a></li></ul></li><li><span class="tocitem">Advanced Topics</span><ul><li><a class="tocitem" href="../initialisation/">Parameter Initialisation</a></li><li class="is-active"><a class="tocitem" href>GPU Usage</a><ul class="internal"><li><a class="tocitem" href="#Prerequisites"><span>Prerequisites</span></a></li><li><a class="tocitem" href="#Basic-GPU-Usage"><span>Basic GPU Usage</span></a></li><li><a class="tocitem" href="#GPU-Performance-Benefits"><span>GPU Performance Benefits</span></a></li><li><a class="tocitem" href="#Performance-Considerations"><span>Performance Considerations</span></a></li><li><a class="tocitem" href="#GPU-vs-CPU-Comparison"><span>GPU vs CPU Comparison</span></a></li></ul></li><li><a class="tocitem" href="../advanced_usage/">Advanced Usage</a></li></ul></li><li><a class="tocitem" href="../api/">API</a></li><li><a class="tocitem" href="../function_index/">Index</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Advanced Topics</a></li><li class="is-active"><a href>GPU Usage</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>GPU Usage</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/JamieMair/SimpleNNs.jl/blob/main/docs/src/gpu_usage.md#" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="GPU-Usage"><a class="docs-heading-anchor" href="#GPU-Usage">GPU Usage</a><a id="GPU-Usage-1"></a><a class="docs-heading-anchor-permalink" href="#GPU-Usage" title="Permalink"></a></h1><p>SimpleNNs.jl provides full GPU support through CUDA.jl, allowing you to train and run inference on NVIDIA GPUs for significantly improved performance.</p><h2 id="Prerequisites"><a class="docs-heading-anchor" href="#Prerequisites">Prerequisites</a><a id="Prerequisites-1"></a><a class="docs-heading-anchor-permalink" href="#Prerequisites" title="Permalink"></a></h2><p>To use GPU functionality, you need to have the following packages installed:</p><pre><code class="language-julia hljs">using Pkg
Pkg.add([&quot;CUDA&quot;, &quot;cuDNN&quot;, &quot;NNlib&quot;])</code></pre><p>These packages must be loaded <strong>before</strong> using SimpleNNs GPU functionality:</p><pre><code class="language-julia hljs">using CUDA
import cuDNN, NNlib # Need to load CUDA, cuDNN and NNlib to enable GPU functionality in SimpleNNs
using SimpleNNs</code></pre><div class="admonition is-info" id="GPU-Requirements-d4ddc60cd8320c76"><header class="admonition-header">GPU Requirements<a class="admonition-anchor" href="#GPU-Requirements-d4ddc60cd8320c76" title="Permalink"></a></header><div class="admonition-body"><p>You need an NVIDIA GPU with CUDA Compute Capability 3.5 or higher. Check your GPU compatibility with <code>CUDA.functional()</code>.</p></div></div><h2 id="Basic-GPU-Usage"><a class="docs-heading-anchor" href="#Basic-GPU-Usage">Basic GPU Usage</a><a id="Basic-GPU-Usage-1"></a><a class="docs-heading-anchor-permalink" href="#Basic-GPU-Usage" title="Permalink"></a></h2><h3 id="Moving-Models-to-GPU"><a class="docs-heading-anchor" href="#Moving-Models-to-GPU">Moving Models to GPU</a><a id="Moving-Models-to-GPU-1"></a><a class="docs-heading-anchor-permalink" href="#Moving-Models-to-GPU" title="Permalink"></a></h3><p>The simplest way to use GPU acceleration is with the <code>gpu</code> function:</p><pre><code class="language-julia hljs"># Create a model on CPU
model = chain(
    Static(784),  # MNIST flattened images
    Dense(128, activation_fn=relu),
    Dense(64, activation_fn=relu),
    Dense(10, activation_fn=identity)
)

# Move to GPU
gpu_model = gpu(model)</code></pre><h3 id="GPU-Data-Transfer"><a class="docs-heading-anchor" href="#GPU-Data-Transfer">GPU Data Transfer</a><a id="GPU-Data-Transfer-1"></a><a class="docs-heading-anchor-permalink" href="#GPU-Data-Transfer" title="Permalink"></a></h3><p>You can move arrays to the GPU using the same <code>gpu</code> function:</p><pre><code class="language-julia hljs"># CPU data
cpu_data = randn(Float32, 784, 100)  # 100 samples

# Move to GPU
gpu_data = gpu(cpu_data)

# You can also use CUDA.cu() directly
gpu_data = CUDA.cu(cpu_data)</code></pre><h3 id="Complete-GPU-Training-Example"><a class="docs-heading-anchor" href="#Complete-GPU-Training-Example">Complete GPU Training Example</a><a id="Complete-GPU-Training-Example-1"></a><a class="docs-heading-anchor-permalink" href="#Complete-GPU-Training-Example" title="Permalink"></a></h3><p>Here&#39;s a complete example showing GPU training:</p><pre><code class="language-julia hljs">using CUDA
import cuDNN, NNlib
using SimpleNNs
using Random

# Check GPU availability
if !CUDA.functional()
    error(&quot;CUDA not available!&quot;)
end

# Create model and move to GPU
model = chain(
    Static(10),
    Dense(32, activation_fn=tanh),
    Dense(16, activation_fn=relu),
    Dense(1, activation_fn=identity)
) |&gt; gpu

# Generate sample data on GPU
batch_size = 128
inputs = CUDA.randn(Float32, 10, batch_size)
targets = CUDA.randn(Float32, 1, batch_size)

# Preallocate buffers
forward_cache = preallocate(model, batch_size)
backward_cache = preallocate_grads(model, batch_size)

# Set inputs and create loss
set_inputs!(forward_cache, inputs)
loss = MSELoss(targets)

# Initialise parameters
Random.seed!(42)
ps = parameters(model)
randn!(ps)
ps .*= 0.1f0

# Setup optimiser
optimiser = AdamOptimiser(backward_cache.parameter_gradients; lr=0.01f0)

# Training loop
for epoch in 1:1000
    forward!(forward_cache, model)
    total_loss = backprop!(backward_cache, forward_cache, model, loss)
    
    # Apply optimiser
    grads = gradients(backward_cache)
    update!(ps, grads, optimiser)
    
    if epoch % 100 == 0
        println(&quot;Epoch $epoch, Loss: $total_loss&quot;)
    end
end</code></pre><h2 id="GPU-Performance-Benefits"><a class="docs-heading-anchor" href="#GPU-Performance-Benefits">GPU Performance Benefits</a><a id="GPU-Performance-Benefits-1"></a><a class="docs-heading-anchor-permalink" href="#GPU-Performance-Benefits" title="Permalink"></a></h2><h3 id="Convolutional-Networks"><a class="docs-heading-anchor" href="#Convolutional-Networks">Convolutional Networks</a><a id="Convolutional-Networks-1"></a><a class="docs-heading-anchor-permalink" href="#Convolutional-Networks" title="Permalink"></a></h3><p>GPU acceleration is particularly beneficial for convolutional networks:</p><pre><code class="language-julia hljs">using CUDA, cuDNN, NNlib
using SimpleNNs
using MLDatasets

# Load MNIST data
dataset = MNIST(:train)
images, labels = dataset[:]

# Reshape and move to GPU
images = reshape(images, 28, 28, 1, size(images, 3)) |&gt; gpu
labels = (labels .+ 1) |&gt; gpu

# Create CNN model on GPU
model = chain(
    Static((28, 28, 1)),
    Conv((5,5), 16, activation_fn=relu),
    MaxPool((2,2)),
    Conv((3,3), 8, activation_fn=relu),
    MaxPool((4,4)),
    Flatten(),
    Dense(10, activation_fn=identity)
) |&gt; gpu

batch_size = 64
forward_cache = preallocate(model, batch_size)
backward_cache = preallocate_grads(model, batch_size)

# Training with GPU acceleration
for epoch in 1:100
    # Select random batch
    batch_indices = rand(1:size(images, 4), batch_size)
    batch_images = view(images, :, :, :, batch_indices)
    batch_labels = view(labels, batch_indices)
    
    set_inputs!(forward_cache, batch_images)
    loss = LogitCrossEntropyLoss(batch_labels, 10)
    
    forward!(forward_cache, model)
    total_loss = backprop!(backward_cache, forward_cache, model, loss)
    
    # Apply gradients (simplified)
    ps = parameters(model)
    grads = gradients(backward_cache)
    
    # Use built-in SGD optimiser for demonstration
    if !@isdefined(sgd_opt)
        sgd_opt = SGDOptimiser(grads; lr=0.001f0, momentum=0.9f0)
    end
    update!(ps, grads, sgd_opt)
end</code></pre><h2 id="Performance-Considerations"><a class="docs-heading-anchor" href="#Performance-Considerations">Performance Considerations</a><a id="Performance-Considerations-1"></a><a class="docs-heading-anchor-permalink" href="#Performance-Considerations" title="Permalink"></a></h2><h2 id="GPU-vs-CPU-Comparison"><a class="docs-heading-anchor" href="#GPU-vs-CPU-Comparison">GPU vs CPU Comparison</a><a id="GPU-vs-CPU-Comparison-1"></a><a class="docs-heading-anchor-permalink" href="#GPU-vs-CPU-Comparison" title="Permalink"></a></h2><p>Here&#39;s a simple benchmark comparing GPU and CPU performance:</p><pre><code class="language-julia hljs">using BenchmarkTools

# Create identical models
cpu_model = chain(Static(100), Dense(200, activation_fn=relu), Dense(1))
gpu_model = gpu(cpu_model)

batch_size = 128
cpu_cache = preallocate(cpu_model, batch_size)
gpu_cache = preallocate(gpu_model, batch_size)

# CPU data
cpu_inputs = randn(Float32, 100, batch_size)
set_inputs!(cpu_cache, cpu_inputs)

# GPU data
gpu_inputs = gpu(cpu_inputs)
set_inputs!(gpu_cache, gpu_inputs)

# Benchmark
println(&quot;CPU Performance:&quot;)
@benchmark forward!($cpu_cache, $cpu_model)

println(&quot;GPU Performance:&quot;)
@benchmark CUDA.@sync forward!($gpu_cache, $gpu_model)</code></pre><p>On my machine, I see the following results</p><pre><code class="nohighlight hljs">CPU Benchmark:
BenchmarkTools.Trial: 10000 samples with 1 evaluation per sample.
 Range (min … max):   96.800 μs … 386.100 μs  ┊ GC (min … max): 0.00% … 0.00%
 Time  (median):     105.900 μs               ┊ GC (median):    0.00%
 Time  (mean ± σ):   111.648 μs ±  18.544 μs  ┊ GC (mean ± σ):  0.00% ± 0.00%

  ▁▄▆██▇▇▆▆▆▅▄▄▃▃▃▂▂▁▂▂▁▂▁▁▁▁       ▁                           ▂
  ████████████████████████████████▇████▇█▇██▇▅▇▆▆▅▆▅▃▅▅▄▅▃▅▅▅▄▃ █
  96.8 μs       Histogram: log(frequency) by time        190 μs &lt;

 Memory estimate: 0 bytes, allocs estimate: 0.

GPU Benchmark:
BenchmarkTools.Trial: 10000 samples with 1 evaluation per sample.
 Range (min … max):  67.000 μs … 619.800 μs  ┊ GC (min … max): 0.00% … 0.00%
 Time  (median):     85.700 μs               ┊ GC (median):    0.00%
 Time  (mean ± σ):   96.932 μs ±  42.793 μs  ┊ GC (mean ± σ):  0.00% ± 0.00%

  ▁▁  █▄
  ███████▇▇▆▅▅▅▅▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▂▂▂▁▂
  67 μs           Histogram: frequency by time          275 μs &lt;

 Memory estimate: 9.58 KiB, allocs estimate: 342.</code></pre><p>You can see these small sizes actually show the GPU and CPU having similar performance. Keep this in mind when you are choosing which device to run your small-medium size neural networks on.</p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../initialisation/">« Parameter Initialisation</a><a class="docs-footer-nextpage" href="../advanced_usage/">Advanced Usage »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.13.0 on <span class="colophon-date" title="Sunday 15 February 2026 11:41">Sunday 15 February 2026</span>. Using Julia version 1.11.9.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
