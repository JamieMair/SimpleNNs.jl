# # CONVOLUTIONS
# function _forward_conv_2d_no_bias(outputs, inputs, kernel, activation_fn = identity)
#     s_x = convert(Int32, size(outputs, 1))
#     s_y = convert(Int32, size(outputs, 2))
#     out_channels = convert(Int32, size(outputs, 3))
#     n = convert(Int32, size(outputs, 4))

#     w = convert(Int32, size(kernel, 1))
#     h = convert(Int32, size(kernel, 2))
#     in_channels = convert(Int32, size(kernel, 3))


#     x = threadIdx().x + (blockIdx().x - 1i32) * blockDim().x
#     y = threadIdx().y + (blockIdx().y - 1i32) * blockDim().y
#     i = threadIdx().z + (blockIdx().z - 1i32) * blockDim().z

#     @inbounds if x <= s_x && y <= s_y && i <= n
#         for c_out in 1i32:out_channels
#             s = zero(eltype(outputs))
#             for c_in in 1i32:in_channels
#                 for ky in 1i32:h
#                     for kx in 1i32:w
#                         s = CUDA.fma(kernel[kx, ky, c_in, c_out], inputs[x + kx - 1i32, y + ky - 1i32, c_in, i], s)
#                     end
#                 end
#             end
#             outputs[x, y, c_out, i] = activation_fn(s)
#         end
#     end

#     nothing
# end
# function _forward_conv_2d(outputs, inputs, kernel, bias, activation_fn = identity)
#     s_x = convert(Int32, size(outputs, 1))
#     s_y = convert(Int32, size(outputs, 2))
#     out_channels = convert(Int32, size(outputs, 3))
#     n = convert(Int32, size(outputs, 4))

#     w = convert(Int32, size(kernel, 1))
#     h = convert(Int32, size(kernel, 2))
#     in_channels = convert(Int32, size(kernel, 3))


#     x = threadIdx().x + (blockIdx().x - 1i32) * blockDim().x
#     y = threadIdx().y + (blockIdx().y - 1i32) * blockDim().y
#     i = threadIdx().z + (blockIdx().z - 1i32) * blockDim().z

#     @inbounds if x <= s_x && y <= s_y && i <= n
#         for c_out in 1i32:out_channels
#             s = bias[c_out]
#             for c_in in 1i32:in_channels
#                 for ky in 1i32:h
#                     for kx in 1i32:w
#                         s = CUDA.fma(kernel[kx, ky, c_in, c_out], inputs[x + kx - 1i32, y + ky - 1i32, c_in, i], s)
#                     end
#                 end
#             end
#             outputs[x, y, c_out, i] = activation_fn(s)
#         end
#     end

#     nothing
# end
# function forward!(output::CuArray, layer::Conv, parameters, input::CuArray)
#     kernel = kernel_weights(layer, parameters)

#     if ndims(kernel)==4
#         # Use custom 2D kernel
#         w, h, _, n = size(output)
#         x_threads = min(32, w)
#         y_threads = min(32, h)
#         z_threads = min(1024 รท x_threads รท y_threads, n)
#         num_threads = (x_threads, y_threads, z_threads)
#         num_blocks = (cld(w, x_threads), cld(h, y_threads), cld(n, z_threads))
#         activation = layer.activation_fn

#         if has_bias(layer)
#             biases = kernel_biases(layer, parameters)
#             @cuda blocks=num_blocks threads=num_threads _forward_conv_2d(output, input, kernel, biases, activation)
#         else
#             @cuda blocks=num_blocks threads=num_threads _forward_conv_2d_no_bias(output, input, kernel, activation)
#         end
#     else
#         kernel = kernel_weights(layer, parameters)
#         @assert !has_bias(layer) "Convolutions with $(ndims(kernel)-2) spatial dimensions with a bias are not supported yet."
        
#         biases = kernel_biases(layer, parameters)
#         conv_params = NNlib.DenseConvDims(size(input), size(kernel); flipkernel=true)
#         activation_fn = if typeof(layer.activation_fn) === typeof(relu)
#             NNlib.relu
#         else
#             identity
#         end
    
#         channel_dim = ndims(output)-1
#         biases = reshape(biases, ntuple(i->i==channel_dim ? size(output, i) : 1, ndims(output)))
#         NNlib.conv_bias_act!(output, input, kernel, conv_params, biases, activation_fn)
    
#         if typeof(activation_fn) != typeof(NNlib.relu) && typeof(activation_fn) != typeof(layer.activation_fn)
#             # Apply activation
#             output .= layer.activation_fn.(output)
#         end
#     end

#     nothing
# end

# # POOLING
# function forward_inner!(output::CuArray, layer::MaxPool, input::CuArray)
#     pool_dims = NNlib.PoolDims(size(input), layer.pool_size; stride=layer.stride)
#     NNlib.maxpool!(output, input, pool_dims)
#     return output
# end